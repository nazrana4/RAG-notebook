{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "y4uYdXkVKisF",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "3ceb713a-b941-43ee-e3e3-59a96dcc9b31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Collecting SQLAlchemy<2.0.36,>=1.4 (from langchain-community)\n",
            "  Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.10)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.6 (from langchain-community)\n",
            "  Downloading langchain-0.3.7-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.15 (from langchain-community)\n",
            "  Downloading langchain_core-0.3.15-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.137)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.6->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.6->langchain-community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2.0.36,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain-community) (2.23.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.2.2)\n",
            "Downloading langchain_community-0.3.5-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.7-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.15-py3-none-any.whl (408 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
            "Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: SQLAlchemy, python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain, langchain-community\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.36\n",
            "    Uninstalling SQLAlchemy-2.0.36:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.36\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.13\n",
            "    Uninstalling langchain-core-0.3.13:\n",
            "      Successfully uninstalled langchain-core-0.3.13\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.4\n",
            "    Uninstalling langchain-0.3.4:\n",
            "      Successfully uninstalled langchain-0.3.4\n",
            "Successfully installed SQLAlchemy-2.0.35 dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.7 langchain-community-0.3.5 langchain-core-0.3.15 marshmallow-3.23.1 mypy-extensions-1.0.0 pydantic-settings-2.6.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.10/dist-packages (0.3.15)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (0.1.137)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (4.12.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core) (3.10.10)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (2.23.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core) (2.2.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (1.2.2)\n",
            "Collecting langchain-google-vertexai\n",
            "  Downloading langchain_google_vertexai-2.0.7-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.70.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-vertexai) (1.70.0)\n",
            "Collecting google-cloud-storage<3.0.0,>=2.18.0 (from langchain-google-vertexai)\n",
            "  Downloading google_cloud_storage-2.18.2-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-vertexai) (0.27.2)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-vertexai) (0.4.0)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain-google-vertexai) (0.3.15)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-google-vertexai) (2.9.2)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (2.19.2)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (1.25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (3.20.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (24.1)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (3.25.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (1.13.0)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (2.0.6)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (0.16)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain-google-vertexai) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain-google-vertexai) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain-google-vertexai) (2.32.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain-google-vertexai) (1.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain-google-vertexai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain-google-vertexai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain-google-vertexai) (0.1.137)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain-google-vertexai) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain-google-vertexai) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain-google-vertexai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain-google-vertexai) (2.23.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (1.65.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (4.9)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (0.13.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.15->langchain-google-vertexai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-vertexai) (3.10.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-vertexai) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=2.18.0->langchain-google-vertexai) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=2.18.0->langchain-google-vertexai) (2.2.3)\n",
            "Requirement already satisfied: numpy<3,>=1.14 in /usr/local/lib/python3.10/dist-packages (from shapely<3.0.0dev->google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (1.26.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (1.2.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (1.16.0)\n",
            "Downloading langchain_google_vertexai-2.0.7-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_storage-2.18.2-py2.py3-none-any.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google-cloud-storage, langchain-google-vertexai\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 2.8.0\n",
            "    Uninstalling google-cloud-storage-2.8.0:\n",
            "      Successfully uninstalled google-cloud-storage-2.8.0\n",
            "Successfully installed google-cloud-storage-2.18.2 langchain-google-vertexai-2.0.7\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "a9464e5b28774496b938aa3033b7917a",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m297.0/298.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.1.0\n"
          ]
        }
      ],
      "source": [
        "! pip install langchain-community\n",
        "! pip install langchain-core\n",
        "! pip install langchain-google-vertexai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "_IJ4gwXkM5X9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN']='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'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UEC6C6igM_H"
      },
      "outputs": [],
      "source": [
        "# import google.generativeai as genai\n",
        "\n",
        "# genai.configure(api_key=\"\")\n",
        "# model = genai.GenerativeModel(\"gemini-1.5-flash\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "DDxkid3MPv75"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "def llm_generate(message):\n",
        "  client = OpenAI(\n",
        "    base_url=\"https://api-inference.huggingface.co/v1/\",\n",
        "    api_key=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
        "  )\n",
        "\n",
        "  messages = message\n",
        "\n",
        "  response = client.chat.completions.create(\n",
        "      model=\"NousResearch/Hermes-3-Llama-3.1-8B\",\n",
        "    messages=messages,\n",
        "    max_tokens=500,\n",
        "    stream=False\n",
        "  )\n",
        "\n",
        "  return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "collapsed": true,
        "id": "AxYb8Oy-UGj9",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from typing import List\n",
        "\n",
        "class MyEmbeddings:\n",
        "        def __init__(self, model):\n",
        "            self.model = SentenceTransformer(model, trust_remote_code=True)\n",
        "\n",
        "        def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "            return [self.model.encode(t).tolist() for t in texts]\n",
        "\n",
        "        def embed_query(self, query: str) -> List[float]:\n",
        "            return self.model.encode([query])[0]\n",
        "\n",
        "embeddings=MyEmbeddings('Alibaba-NLP/gte-large-en-v1.5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "JsEan5ClVjgQ",
        "outputId": "b9023492-fd54-499d-ad0e-f89b9c929ca2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'source': '/content/drive/MyDrive/content/Sarang Punekar, first transgender student of SPPU in Pune, dies by suicide _ Pune News - The Indian Express.txt'}, page_content=\"skip to content\\nAdvertisement\\nSarang Punekar, first transgender student of SPPU in Pune, dies by suicide\\nIn Rajasthan, she was living among the transgender community there and worked for them. “We had asked her to come back,” said Pune-based freelance writer Ashwini Satav, who worked with her for a long time.\\nWritten by\\nParthasarathi Biswas\\nPune |\\nUpdated: January 17, 2025 05:19 IST\\n3 min read\\nPunekar was a strong supporter of the Ambedkarite movement and also raised her voice against NRC and CAA. “Sarang was brilliant in her analysis of caste and power hierarchy.\\nThirty-year-old Sarang Punekar, the first transgender student of Savitribai Phule Pune University (SPPU), died by suicide in Rajasthan on Wednesday. Her last rites were performed in Pune on Thursday.\\nIn\\nRajasthan\\n, she was living among the transgender community there and worked for them. “We had asked her to come back,” said\\nPune\\n-based freelance writer Ashwini Satav, who worked with her for a long time.\\nPunekar was a strong supporter of the Ambedkarite movement and also raised her voice against\\nNRC\\nand CAA. “Sarang was brilliant in her analysis of caste and power hierarchy. As the first transgender student of the university, her very presence was a unique experience for us, both as academics as well as administrators,” Dr Anagha Tambe, head of the Department of Women’s Studies at SPPU which Punekar had joined in 2018, said.\\nAdvertisement\\nTambe, while speaking to\\nThe Indian Express\\non Thursday at Punekar’s funeral, felt society at large failed to help Punekar realise her dreams.\\nShiv Sena\\n(UBT) leader Sushma Andhare was among those who had gathered to bid farewell to Punekar, who had made a name for herself within a short time as a passionate speaker and advocate for gender rights and other causes. “As a student, Punekar brought about new perspectives to gender studies. She wanted to create knowledge and wanted to do original work about the language and customs of her community. It is our failure as a society that we could not support her dreams,” she said.\\nPunekar had worked as a regional coordinator, north\\nMaharashtra\\n, for Pune-headquartered NGO Samyak, which works for women and sexuality. Anand Pawar, executive director of the NGO, said she was first referred to Samyak as an intern but subsequently she was absorbed. “In the development sector it is always the norm to get transgender people to work in HIV prevention programmes. But she broke the stereotype and worked for abortion rights. As the coordinator she executed the project successfully,” he said.\\nIn the course of her work, she had to coordinate with government officials, doctors and NGOs. The project got over in 2020 and Punekar decided to relocate to Rajasthan. “She said she wanted to be with the community there,” Pawar said.\\nAdvertisement\\nTransgender poetess Disha Pinky Sheikh was her professor and personal friend. “She was a very strong voice. Her death marks the end of a very different kind of support for the community,” she said.\\nDiscover the Benefits of Our Subscription!\\nStay informed with access to our award-winning journalism.\\nAvoid misinformation with trusted, accurate reporting.\\nMake smarter decisions with insights that matter.\\nChoose your subscription package\\nRecommended\\nAll-Access\\nDigital + E-paper subscription so that you don’t miss anything.\\nRs.1299/year\\nBUY NOW\\nUPSC Special\\nAll-access plus monthly magazine for competitive exam prep.\\nRs.1499/year\\nBUY NOW\\nClick here\\nto join\\nExpress Pune WhatsApp channel\\nand get a curated list of our stories\\n© The Indian Express Pvt Ltd\\nParthasarathi Biswas\\nPartha Sarathi Biwas is an Assistant Editor with The Indian Express with 10+ years of experience in reporting on Agriculture, Commodities and Developmental issues. He has been with The Indian Express since 2011 and earlier worked with DNA. Partha's report about Farmers Producer Companies (FPC) as well long pieces on various agricultural issues have been cited by various academic publications including those published by the Government of India. He is often invited as a visiting faculty to various schools of journalism to talk about development journalism and rural reporting. In his spare time Partha trains for marathons and has participated in multiple marathons and half marathons.\\n... Read More\\nLatest Comment\\nPost Comment\\nRead Comments\\nAdvertisement\\nSuspect detained in attack on Saif Ali Khan, officers say being questioned\\nCities\\n38 min ago\\nA suspect has been detained by the Bandra police in connection with the attack on actor Saif Ali Khan at his residence. 20 police teams are working on the case, with one focusing on leads from informers and analyzing their database. The attacker, believed to be in his 30s, demanded money and injured the family's nanny before attacking Khan with a blade.\\nView all shorts\\nLive\\nBlog\\nMahakumbh Mela 2025 Live Updates: ‘A celebration of unity’ — BJP distributes Constitution copies to sanitation workers\\n1 hour ago\\nDelhi News Live Updates: Last day to file nominations for Assembly polls today; BJP likely to release manifesto\\n32 mins ago\\nDelhi Nursery Admission 2025 1st Merit List Live Updates: When is merit list releasing?\\n13 mins ago\\nSaif Ali Khan Attacked News Live Updates: Kareena, Karisma, Ranbir meet Saif in hospital, ‘Saif out of danger,’ says doctor\\n8 hours ago\\nTop Stories\\nCities\\nLive: Cops detain suspect in Saif Ali Khan attack case, taken to Bandra police station\\nFormer Pakistan PM Imran Khan, wife Bushra Bibi convicted in £190 million graft case\\nEntertainment\\nEmergency movie review: Kangana Ranaut's confused Indira Gandhi biopic is weak in craft\\nEntertainment\\nPataal Lok 2 review: Sharp and searing, Jaideep Ahlawat-Sudip Sharma deliver one of the best shows of 2025\\nTrending\\nWho is ‘Mahakumbh ki Mona Lisa’? Know why the Internet is divided over a garland seller\\nTrending\\nChris Martin soaks in Marine Drive vibes as Coldplay arrives in Mumbai for India tour: 'Grateful to be here'\\nSports\\nBCCI issues curbs on personal staff, family travel, endorsement shoots during tours for centrally-contracted cricketers; violators may face IPL ban\\nSports\\nHockey India League: From losing vision to winning Olympic gold - Jip Janssen's inspirational story\\nOpinion\\nIsrael-Hamas ceasefire is welcome – but peace won’t be easy\\nExplained\\nWhat are the key takeaways from the Israel-Hamas ceasefire agreement?\\nLifestyle\\nDiabetics, here's what happens to the body if you skip breakfast\\nTechnology\\nNintendo Switch 2 trailer: Every little detail you might have missed (and what we still don’t know)\\nAdvertisement\\nMust Read\\nSports\\nBCCI issues curbs on personal staff, family travel, endorsement shoots during tours for centrally-contracted cricketers; violators may face IPL ban\\nSports\\nHockey India League: From losing vision to winning Olympic gold - Jip Janssen's inspirational story\\nSports\\nWPL 2025: Baroda, Bengaluru, Lucknow and Mumbai set to host 3rd edition of league\\nTechnology\\nNintendo Switch 2 trailer: Every little detail you might have missed (and what we still don’t know)\\nTechnology\\nGoogle refuses to embed fact-checking into core search algorithms: Report\\nTechnology\\nApple deepens its India focus with an online store app\\nLifestyle\\nDiabetics, here's what happens to the body if you skip breakfast\\nAdvertisement\\nBuzzing Now\\nTrending\\nWho is ‘Mahakumbh ki Mona Lisa’? Know why the Internet is divided over a garland seller\\nTrending\\nChris Martin soaks in Marine Drive vibes as Coldplay arrives in Mumbai for India tour: 'Grateful to be here'\\nTrending\\nSchoolboy's electrifying dance to Devara's 'Davaudi' goes viral; earns praise from Jr NTR\\nTrending\\nA shoe or a clutch? Influencer reacts to Balenciaga’s new product\\nTrending\\nRam Gopal Varma shares AI video reimagining 'The Substance' with him, Sridevi and Jahnvi Kapoor: 'AI is becoming too much'\\nJan 17:\\nLatest News\\n01\\nPune’s Jewish community heaves a sigh of relief as Israel & Hamas agree to a ceasefire\\n02\\nWorking on new start-up policy, plans to set up ‘Innovation City’: CM Devendra Fadnavis\\n03\\nVijay Hazare Trophy: Karun Nair gives selectors more food for thought as Vidarbha thrash Maharashtra to enter final\\n04\\nEmergency brazenly undermined Dr B R Ambedkar’s Indian Constitution: CM Devendra Fadnavis\\n05\\nPune firm CEO lured by ‘Facebook friend’, loses Rs 42 lakh in share trading fraud\\nAdvertisement\\nLink Subscription\\nSubscribe\\nSign In\\ne-paper\\nPremium\\nIndia\\nElections 2024\\nBollywood\\nOpinion\\nPolitical Pulse\\nExplained\\nScience\\nCricket\\nSports\\nWorld\\nBusiness\\nEntertainment\\nJobs\\nHealth\\nLifestyle\\nTechnology\\nEducation\\nMovie Review\\nEye\\nTrending\\nCities\\nNewsletters\\nWebSeries\\nPhotos\\nVideos\\nAudio\\nWeb Stories\\nTrending\\nSmart Stocks\\nMini Crossword\\nPremium\\nExpress Shorts\\n🎙️ Podcast\\nHealth & Wellness\\nclose\\nEdition\\nIndia\\nInternational\\n.(Open in new tab)\\nSubscribe\\nSign In\\ne-paper\\nPremium\\nIndia\\nElections 2024\\nBollywood\\nOpinion\\nPolitical Pulse\\nExplained\\nScience\\nCricket\\nSports\\nWorld\\nBusiness\\nEntertainment\\nJobs\\nHealth\\nLifestyle\\nTechnology\\nEducation\\nMovie Review\\nEye\\nTrending\\nCities\\nNewsletters\\nWebSeries\\nPhotos\\nVideos\\nAudio\\nWeb Stories\\nTop Categories\\nExplained News\\nPolitical Pulse\\nLatest Opinion\\nMumbai News\\nDelhi News\\nPune News\\nBangalore News\\nBollywood News\\nHealth News\\nIndia News\\nSports News\\nLifestyle News\\nLatest News\\nCricket\\nTech Reviews\\nGadgets\\nMobile & Tabs\\nFood & Wine\\nElections 2024\\nFitness\\nTrending News\\nMumbai News\\nLos Angeles Wildfires\\nJoe Biden Farewell\\nJubilant Foodworks Stock Analysis\\nHoroscope Today\\nJio Prepaid Recharge Plans\\nAirtel Prepaid Recharge Plans\\nStock Analysis\\nGT Team 2025 Players List\\nLive Cricket Score\\nScreen Videos\\nStock Market\\nInternational Videos\\nIndian Express Live TV\\nLATEST STORIES\\nJallikattu, manjuvirattu events leave seven dead, scores injured in Tamil Nadu\\nKangana Ranaut on terming Vikrant Massey ‘a good actor’ after once threatening to hit him with chappal: ‘Who am I to judge him?’\\nChina launches Pakistani satellite into space\\nColdplay Concerts: Schedule, venues, tickets and all you need to know about band’s ‘Music of the Spheres’ World Tour\\nFormer Pakistan PM Imran Khan, wife Bushra Bibi convicted in £190 million graft case\\nKarnataka vs Vidarbha, Vijay Hazare Trophy 2024-25 Final, Live Streaming: When and where to watch live?\\nSupreme Court says abetment to suicide charges should not be invoked ‘mechanically’\\nIt was in Netanyahu’s interest to wait for Trump before agreeing to a ceasefire\\nA judge in Texas rules 3 other states can challenge access to abortion pill mifepristone nationwide\\nSuspect detained in attack on Saif Ali Khan, officers say being questioned\\nAmid Centre’s bid to tighten Indo-Myanmar border, why Mizoram, Nagaland are silent\\nTennis Legend lost her trophies while evacuating during LA fires\\nIndian national sentenced to 8 years for attempted White House attack\\nGoogle refuses to embed fact-checking into core search algorithms: Report\\nKejriwal writes to PM Modi, seeks discount for students in Delhi Metro trains\\nBengaluru may see cooler weather this weekend, today’s temperature likely to drop to 14.3 degrees\\nFollow Us\\nDownload Apps\\nThe Indian Express website has been rated GREEN for its credibility and trustworthiness by Newsguard, a global service that rates news sources for their journalistic standards.\\nExpress Group\\nThe Indian Express\\nieTamil.com\\nThe Financial Express\\nieBangla.com\\nLoksatta\\nieMalayalam.com\\nJansatta\\nieGujarati.com\\ninUth\\nIE Education\\nThe ExpressGroup\\nNewsletters\\n26/11 Stories of Strength\\nRamnath Goenka Excellence in Journalism Awards\\nLight House Journalism\\nCareers\\nQuick Links\\nT&C\\nPrivacy Policy\\nAdvertise with Us\\nBrand Solutions\\nContact Us\\nSubscribe\\nStatutory provisions on reporting (sexual offenses)\\nThis website follows the DNPA’s code of conduct\\nCSR\\nCopyright © 2025 The Indian Express [P] Ltd. All Rights Reserved\\nOur award-winning news reports, explainers now straight on your device\\nOpt-in to receive alerts via WhatsApp and SMS\\nThis No Is Already Registered.\\nThanks For Registered Mobile No.\\nSave\"), Document(metadata={'source': \"/content/drive/MyDrive/content/BJP's Delhi polls manifesto out, Arvind Kejriwal attacks PM Modi on 'revdis' - India Today.txt\"}, page_content='India Today\\nAaj Tak\\nGNTTV\\nLallantop\\nBusiness Today\\nBangla\\nMalayalam\\nNortheast\\nBT Bazaar\\nHarper\\'s Bazaar\\nSports Tak\\nCrime Tak\\nAstro Tak\\nGaming\\nBrides Today\\nCosmopolitan\\nKisan Tak\\nIshq FM\\nIndia Today Hindi\\nReader’s Digest\\nAaj Tak Campus\\nIndia Today\\nAaj Tak\\nGNTTV\\nLallantop\\nBusiness Today\\nBangla\\nMalayalam\\nNortheast\\nBT Bazaar\\nHarper\\'s Bazaar\\nSports Tak\\nSIGN IN\\nEdition\\nIN\\nIN\\nUS\\nDownload App\\nFollow Us On:\\nNews\\nElections\\nAssembly\\nDelhi\\n\\'Did PM approve revdi distribution?\\' Arvind Kejriwal attacks BJP manifesto\\n\\'Did PM approve revdi distribution?\\' Arvind Kejriwal attacks BJP manifesto\\nArvind Kejriwal sought PM Modi\\'s justification on \"free revdis\" in the BJP manifesto for Delhi polls. Hitting back, the BJP said Kejriwal wouldn\\'t understand the difference between revdi and development.\\nListen to Story\\nLive TV\\nShare\\nAdvertisement\\nArvind Kejriwal (L) questioned PM Modi after the BJP released its manifesto for the Delhi polls.\\nAmit Bhardwaj\\nNew Delhi\\n,\\nUPDATED:\\nJan 17, 2025 17:34 IST\\nEdited By:\\nPoorva Joshi\\nIn Short\\nRs 21,000 for pregnant women, Rs 500 LPG subsidy among BJP\\'s promises\\nArvind Kejriwal questions BJP manifesto, asks PM\\'s justification on \\'revdis\\'\\nPM, other BJP leaders have attacked \\'free revdi\\' culture for votes in the past\\nAttacking the BJP over its\\nmanifesto for the Delhi Assembly elections\\n, AAP chief Arvind Kejriwal asked if Prime Minister Narendra Modi\\'s permission was taken for the \"distribution of free revdi (sweets)\". The BJP also hit back, saying Kejriwal, \"a criminal out on bail\", would not understand the difference between revdi and development.\\nKejriwal\\'s remarks came not so long after the BJP released its poll manifesto, promising Rs 21,000 for pregnant women, Rs 2,500 per month to women voters, and a Rs 500 subsidy for LPG cylinders in Delhi.\\nadvertisement\\n\"BJP President JP Nadda issued the party\\'s sankalp patra, and announced several revdis. My question is did he take PM Modi’s approval for distributing these revdis?\" Kejriwal said in a press conference.\\n\"PM Modi has said at least 100 times that I distribute free ki revdi, which is not good for the nation. Now, the PM needs to give a justification (on BJP\\'s manifesto). He should accept that he was wrong and Arvind Kejriwal is right,\" the AAP chief added.\\nHitting back at Arvind Kejriwal, Delhi BJP President Virendra Sachdeva quipped, \"When you speak, it sounds like you\\'re giving things out for free. However, our PM speaks of holistic development by taking the entire society, and uplifting every person of the society through facilities.\"\\nKejriwal\\'s attack came against the backdrop of PM Modi and BJP leaders\\' criticisms of AAP\\'s initiatives in Delhi as \\'free revdi\\'. In the past,\\nPM Modi has cautioned people\\nagainst the \\'revdi culture\\' under which votes were sought.\\nBJP\\'s Sankalp Patra was released by JP Nadda today ahead of the February 5 polls in Delhi.\\nRs 5 lakh additional cover under Ayushman Bharat - the Centre\\'s insurance scheme that provides health cover of Rs 5 lakh per family per year, was also among the schemes announced.\\n\"The BJP will enforce the centre\\'s flagship Ayushman Bharat health scheme in its first Cabinet meeting. The AAP has been opposing the scheme in Delhi,\" Nadda said.\\nThe manifesto mostly focusses on the Delhi youth and women. The Rs 2,500 per month announcement for women came on the lines of the Ladki Bahin Yojana, which helped the BJP win the Maharashtra elections.\\nPublished By:\\nPoorva Joshi\\nPublished On:\\nJan 17, 2025\\nMust Watch\\nWatch Live TV\\nAdvertisement\\nAlso Watch\\nSaif Ali Khan\\'s attacker seen entering his apartment in video\\nSupreme Court pauses order on Ayushman Bharat scheme\\'s implementation in Delhi\\nVideo: Kareena Kapoor arrives at Lilavati hospital to meet Saif Ali Khan\\nJaved Akhtar’s journey from \\'Jaadu\\' to legendary poet\\nPolice detain suspect in connection with Saif Ali Khan attack case\\nAdvertisement\\nRead This\\nSuzuki e-Access, new Access 125, Gixxer SF 250 Flex Fuel launched, check all details\\nIndia to announce Champions Trophy squad tomorrow, Ajit Agarkar to address press\\n\\'Did PM approve revdi distribution?\\' Arvind Kejriwal attacks BJP manifesto\\niPhone 17 Air launching in 2025? A19 chip, 120Hz refresh, slim design and everything we expect\\nWhat is CBAM and its impact on Indian exports and sustainability\\nAdvertisement\\nFollow Us On:\\nAdvertisement\\nPUBLICATIONS\\nIndia Today\\nBusiness Today\\nIndia Today-Hindi\\nTIME\\nTELEVISION\\nIndia Today TV\\nAaj Tak\\nGood News Today\\nEVENTS\\nAgenda AajTak\\nIndia Today Conclave\\nSahitya AajTak\\nRADIO\\nIshq FM\\nAajTak Radio\\nGAMING\\nIndia Today Gaming\\nWorld Esports Cup\\nUSEFUL LINKS\\nPress Release\\nSitemap\\nNews\\nNewsletter\\nPrivacy Policy\\nCorrection Policy\\nLMIL Documents\\nPRINTING\\nThomson Press\\nWELFARE\\nCare Today\\nDISTRIBUTION\\nRate Card\\nSYNDICATIONS\\nHeadlines Today\\nWEBSITES\\nIndia Today\\nIndia Today Malayalam\\nIndia Today NE\\nBusiness Today\\nDailyO\\nAajTak\\nLallantop\\nBangla\\nGNTTV\\niChowk\\nReader’s Digest\\nCosmopolitan\\nEDUCATION\\nVasant Valley\\nBest Colleges\\nBest Universities\\nTRENDING TOPICS\\nAtishi\\nHMPV Virus\\nStock Market Today\\nStocks To Watch Today\\nDelhi Air Pollution\\nJustin Trudeau\\nDelhi Election Date\\nEarthquake Today\\nBudget 2025\\nLATEST\\nDelhi Election Dates\\nHoroscope Today\\nDownload App\\nABOUT US\\nCONTACT US\\nTERMS AND CONDITIONS\\nARCHIVES\\nCopyright ©\\n2025\\nLiving Media India Limited. For reprint rights:\\nSyndications Today'), Document(metadata={'source': '/content/drive/MyDrive/content/Beautiful Soup Documentation — Beautiful Soup 4.12.0 documentation.txt'}, page_content='Navigation\\nindex\\nmodules\\n|\\nBeautiful Soup 4.12.0 documentation\\n»\\nBeautiful Soup Documentation\\nBeautiful Soup Documentation\\n¶\\nBeautiful Soup\\nis a\\nPython library for pulling data out of HTML and XML files. It works\\nwith your favorite parser to provide idiomatic ways of navigating,\\nsearching, and modifying the parse tree. It commonly saves programmers\\nhours or days of work.\\nThese instructions illustrate all major features of Beautiful Soup 4,\\nwith examples. I show you what the library is good for, how it works,\\nhow to use it, how to make it do what you want, and what to do when it\\nviolates your expectations.\\nThis document covers Beautiful Soup version 4.12.2. The examples in\\nthis documentation were written for Python 3.8.\\nYou might be looking for the documentation for\\nBeautiful Soup 3\\n.\\nIf so, you should know that Beautiful Soup 3 is no longer being\\ndeveloped and that all support for it was dropped on December\\n31, 2020. If you want to learn about the differences between Beautiful\\nSoup 3 and Beautiful Soup 4, see\\nPorting code to BS4\\n.\\nThis documentation has been translated into other languages by\\nBeautiful Soup users:\\n这篇文档当然还有中文版.\\nこのページは日本語で利用できます(\\n外部リンク\\n)\\n이 문서는 한국어 번역도 가능합니다.\\nEste documento também está disponível em Português do Brasil.\\nEste documento también está disponible en una traducción al español.\\nЭта документация доступна на русском языке.\\nGetting help\\n¶\\nIf you have questions about Beautiful Soup, or run into problems,\\nsend mail to the discussion group\\n. If\\nyour problem involves parsing an HTML document, be sure to mention\\nwhat the diagnose() function says\\nabout\\nthat document.\\nWhen reporting an error in this documentation, please mention which\\ntranslation you’re reading.\\nQuick Start\\n¶\\nHere’s an HTML document I’ll be using as an example throughout this\\ndocument. It’s part of a story from\\nAlice in Wonderland\\n:\\nhtml_doc\\n=\\n\"\"\"<html><head><title>The Dormouse\\'s story</title></head>\\n<body>\\n<p class=\"title\"><b>The Dormouse\\'s story</b></p>\\n<p class=\"story\">Once upon a time there were three little sisters; and their names were\\n<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\\n<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\\n<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\\nand they lived at the bottom of a well.</p>\\n<p class=\"story\">...</p>\\n\"\"\"\\nRunning the “three sisters” document through Beautiful Soup gives us a\\nBeautifulSoup\\nobject, which represents the document as a nested\\ndata structure:\\nfrom\\nbs4\\nimport\\nBeautifulSoup\\nsoup\\n=\\nBeautifulSoup\\n(\\nhtml_doc\\n,\\n\\'html.parser\\'\\n)\\nprint\\n(\\nsoup\\n.\\nprettify\\n())\\n# <html>\\n# <head>\\n# <title>\\n# The Dormouse\\'s story\\n# </title>\\n# </head>\\n# <body>\\n# <p class=\"title\">\\n# <b>\\n# The Dormouse\\'s story\\n# </b>\\n# </p>\\n# <p class=\"story\">\\n# Once upon a time there were three little sisters; and their names were\\n# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\\n# Elsie\\n# </a>\\n# ,\\n# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\\n# Lacie\\n# </a>\\n# and\\n# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">\\n# Tillie\\n# </a>\\n# ; and they lived at the bottom of a well.\\n# </p>\\n# <p class=\"story\">\\n# ...\\n# </p>\\n# </body>\\n# </html>\\nHere are some simple ways to navigate that data structure:\\nsoup\\n.\\ntitle\\n# <title>The Dormouse\\'s story</title>\\nsoup\\n.\\ntitle\\n.\\nname\\n# u\\'title\\'\\nsoup\\n.\\ntitle\\n.\\nstring\\n# u\\'The Dormouse\\'s story\\'\\nsoup\\n.\\ntitle\\n.\\nparent\\n.\\nname\\n# u\\'head\\'\\nsoup\\n.\\np\\n# <p class=\"title\"><b>The Dormouse\\'s story</b></p>\\nsoup\\n.\\np\\n[\\n\\'class\\'\\n]\\n# u\\'title\\'\\nsoup\\n.\\na\\n# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\\nsoup\\n.\\nfind_all\\n(\\n\\'a\\'\\n)\\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\\n# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\\n# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\\nsoup\\n.\\nfind\\n(\\nid\\n=\\n\"link3\"\\n)\\n# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\\nOne common task is extracting all the URLs found within a page’s <a> tags:\\nfor\\nlink\\nin\\nsoup\\n.\\nfind_all\\n(\\n\\'a\\'\\n):\\nprint\\n(\\nlink\\n.\\nget\\n(\\n\\'href\\'\\n))\\n# http://example.com/elsie\\n# http://example.com/lacie\\n# http://example.com/tillie\\nAnother common task is extracting all the text from a page:\\nprint\\n(\\nsoup\\n.\\nget_text\\n())\\n# The Dormouse\\'s story\\n#\\n# The Dormouse\\'s story\\n#\\n# Once upon a time there were three little sisters; and their names were\\n# Elsie,\\n# Lacie and\\n# Tillie;\\n# and they lived at the bottom of a well.\\n#\\n# ...\\nDoes this look like what you need? If so, read on.\\nInstalling Beautiful Soup\\n¶\\nIf you’re using a recent version of Debian or Ubuntu Linux, you can\\ninstall Beautiful Soup with the system package manager:\\n$\\napt\\n-\\nget\\ninstall\\npython3\\n-\\nbs4\\nBeautiful Soup 4 is published through PyPi, so if you can’t install it\\nwith the system packager, you can install it with\\neasy_install\\nor\\npip\\n. The package name is\\nbeautifulsoup4\\n. Make sure you use the\\nright version of\\npip\\nor\\neasy_install\\nfor your Python version\\n(these may be named\\npip3\\nand\\neasy_install3\\nrespectively).\\n$\\neasy_install\\nbeautifulsoup4\\n$\\npip\\ninstall\\nbeautifulsoup4\\n(The\\nBeautifulSoup\\npackage is\\nnot\\nwhat you want. That’s\\nthe previous major release,\\nBeautiful Soup 3\\n. Lots of software uses\\nBS3, so it’s still available, but if you’re writing new code you\\nshould install\\nbeautifulsoup4\\n.)\\nIf you don’t have\\neasy_install\\nor\\npip\\ninstalled, you can\\ndownload the Beautiful Soup 4 source tarball\\nand\\ninstall it with\\nsetup.py\\n.\\n$\\npython\\nsetup.py\\ninstall\\nIf all else fails, the license for Beautiful Soup allows you to\\npackage the entire library with your application. You can download the\\ntarball, copy its\\nbs4\\ndirectory into your application’s codebase,\\nand use Beautiful Soup without installing it at all.\\nI use Python 3.10 to develop Beautiful Soup, but it should work with\\nother recent versions.\\nInstalling a parser\\n¶\\nBeautiful Soup supports the HTML parser included in Python’s standard\\nlibrary, but it also supports a number of third-party Python parsers.\\nOne is the\\nlxml parser\\n. Depending on your setup,\\nyou might install lxml with one of these commands:\\n$\\napt\\n-\\nget\\ninstall\\npython\\n-\\nlxml\\n$\\neasy_install\\nlxml\\n$\\npip\\ninstall\\nlxml\\nAnother alternative is the pure-Python\\nhtml5lib parser\\n, which parses HTML the way a\\nweb browser does. Depending on your setup, you might install html5lib\\nwith one of these commands:\\n$\\napt\\n-\\nget\\ninstall\\npython3\\n-\\nhtml5lib\\n$\\npip\\ninstall\\nhtml5lib\\nThis table summarizes the advantages and disadvantages of each parser library:\\nParser\\nTypical usage\\nAdvantages\\nDisadvantages\\nPython’s html.parser\\nBeautifulSoup(markup,\\n\"html.parser\")\\nBatteries included\\nDecent speed\\nNot as fast as lxml,\\nless lenient than\\nhtml5lib.\\nlxml’s HTML parser\\nBeautifulSoup(markup,\\n\"lxml\")\\nVery fast\\nExternal C dependency\\nlxml’s XML parser\\nBeautifulSoup(markup,\\n\"lxml-xml\")\\nBeautifulSoup(markup,\\n\"xml\")\\nVery fast\\nThe only currently supported\\nXML parser\\nExternal C dependency\\nhtml5lib\\nBeautifulSoup(markup,\\n\"html5lib\")\\nExtremely lenient\\nParses pages the same way a\\nweb browser does\\nCreates valid HTML5\\nVery slow\\nExternal Python\\ndependency\\nIf you can, I recommend you install and use lxml for speed.\\nNote that if a document is invalid, different parsers will generate\\ndifferent Beautiful Soup trees for it. See\\nDifferences\\nbetween parsers\\nfor details.\\nMaking the soup\\n¶\\nTo parse a document, pass it into the\\nBeautifulSoup\\nconstructor. You can pass in a string or an open filehandle:\\nfrom\\nbs4\\nimport\\nBeautifulSoup\\nwith\\nopen\\n(\\n\"index.html\"\\n)\\nas\\nfp\\n:\\nsoup\\n=\\nBeautifulSoup\\n(\\nfp\\n,\\n\\'html.parser\\'\\n)\\nsoup\\n=\\nBeautifulSoup\\n(\\n\"<html>a web page</html>\"\\n,\\n\\'html.parser\\'\\n)\\nFirst, the document is converted to Unicode, and HTML entities are\\nconverted to Unicode characters:\\nprint\\n(\\nBeautifulSoup\\n(\\n\"<html><head></head><body>Sacr&eacute; bleu!</body></html>\"\\n,\\n\"html.parser\"\\n))\\n# <html><head></head><body>Sacré bleu!</body></html>\\nBeautiful Soup then parses the document using the best available\\nparser. It will use an HTML parser unless you specifically tell it to\\nuse an XML parser. (See\\nParsing XML\\n.)\\nKinds of objects\\n¶\\nBeautiful Soup transforms a complex HTML document into a complex tree\\nof Python objects. But you’ll only ever have to deal with about four\\nkinds\\nof objects:\\nTag\\n,\\nNavigableString\\n,\\nBeautifulSoup\\n,\\nand\\nComment\\n. These objects represent the HTML\\nelements\\nthat comprise the page.\\nclass\\nbs4.\\nTag\\n¶\\nA\\nTag\\nobject corresponds to an XML or HTML tag in the original document.\\nsoup\\n=\\nBeautifulSoup\\n(\\n\\'<b class=\"boldest\">Extremely bold</b>\\'\\n,\\n\\'html.parser\\'\\n)\\ntag\\n=\\nsoup\\n.\\nb\\ntype\\n(\\ntag\\n)\\n# <class \\'bs4.element.Tag\\'>\\nTags have a lot of attributes and methods, and I’ll cover most of them\\nin\\nNavigating the tree\\nand\\nSearching the tree\\n. For now, the most\\nimportant methods of a tag are for accessing its name and attributes.\\nname\\n¶\\nEvery tag has a name:\\ntag\\n.\\nname\\n# \\'b\\'\\nIf you change a tag’s name, the change will be reflected in any\\nmarkup generated by Beautiful Soup down the line:\\ntag\\n.\\nname\\n=\\n\"blockquote\"\\ntag\\n# <blockquote class=\"boldest\">Extremely bold</blockquote>\\nattrs\\n¶\\nAn HTML or XML tag may have any number of attributes. The tag\\n<b\\nid=\"boldest\">\\nhas an attribute “id” whose value is\\n“boldest”. You can access a tag’s attributes by treating the tag like\\na dictionary:\\ntag\\n=\\nBeautifulSoup\\n(\\n\\'<b id=\"boldest\">bold</b>\\'\\n,\\n\\'html.parser\\'\\n)\\n.\\nb\\ntag\\n[\\n\\'id\\'\\n]\\n# \\'boldest\\'\\nYou can access the dictionary of attributes directly as\\n.attrs\\n:\\ntag\\n.\\nattrs\\n# {\\'id\\': \\'boldest\\'}\\ntag\\n.\\nattrs\\n.\\nkeys\\n()\\n# dict_keys([\\'id\\'])\\nYou can add, remove, and modify a tag’s attributes. Again, this is\\ndone by treating the tag as a dictionary:\\ntag\\n[\\n\\'id\\'\\n]\\n=\\n\\'verybold\\'\\ntag\\n[\\n\\'another-attribute\\'\\n]\\n=\\n1\\ntag\\n# <b another-attribute=\"1\" id=\"verybold\"></b>\\ndel\\ntag\\n[\\n\\'id\\'\\n]\\ndel\\ntag\\n[\\n\\'another-attribute\\'\\n]\\ntag\\n# <b>bold</b>\\ntag\\n[\\n\\'id\\'\\n]\\n# KeyError: \\'id\\'\\ntag\\n.\\nget\\n(\\n\\'id\\'\\n)\\n# None\\nMulti-valued attributes\\n¶\\nHTML 4 defines a few attributes that can have multiple values. HTML 5\\nremoves a couple of them, but defines a few more. The most common\\nmulti-valued attribute is\\nclass\\n(that is, a tag can have more than\\none CSS class). Others include\\nrel\\n,\\nrev\\n,\\naccept-charset\\n,\\nheaders\\n, and\\naccesskey\\n. By default, Beautiful Soup stores the value(s)\\nof a multi-valued attribute as a list:\\ncss_soup\\n=\\nBeautifulSoup\\n(\\n\\'<p class=\"body\"></p>\\'\\n,\\n\\'html.parser\\'\\n)\\ncss_soup\\n.\\np\\n[\\n\\'class\\'\\n]\\n# [\\'body\\']\\ncss_soup\\n=\\nBeautifulSoup\\n(\\n\\'<p class=\"body strikeout\"></p>\\'\\n,\\n\\'html.parser\\'\\n)\\ncss_soup\\n.\\np\\n[\\n\\'class\\'\\n]\\n# [\\'body\\', \\'strikeout\\']\\nWhen you turn a tag back into a string, the values of any multi-valued\\nattributes are consolidated:\\nrel_soup\\n=\\nBeautifulSoup\\n(\\n\\'<p>Back to the <a rel=\"index first\">homepage</a></p>\\'\\n,\\n\\'html.parser\\'\\n)\\nrel_soup\\n.\\na\\n[\\n\\'rel\\'\\n]\\n# [\\'index\\', \\'first\\']\\nrel_soup\\n.\\na\\n[\\n\\'rel\\'\\n]\\n=\\n[\\n\\'index\\'\\n,\\n\\'contents\\'\\n]\\nprint\\n(\\nrel_soup\\n.\\np\\n)\\n# <p>Back to the <a rel=\"index contents\">homepage</a></p>\\nIf an attribute\\nlooks\\nlike it has more than one value, but it’s not\\na multi-valued attribute as defined by any version of the HTML\\nstandard, Beautiful Soup stores it as a simple string:\\nid_soup\\n=\\nBeautifulSoup\\n(\\n\\'<p id=\"my id\"></p>\\'\\n,\\n\\'html.parser\\'\\n)\\nid_soup\\n.\\np\\n[\\n\\'id\\'\\n]\\n# \\'my id\\'\\nYou can force all attributes to be stored as strings by passing\\nmulti_valued_attributes=None\\nas a keyword argument into the\\nBeautifulSoup\\nconstructor:\\nno_list_soup\\n=\\nBeautifulSoup\\n(\\n\\'<p class=\"body strikeout\"></p>\\'\\n,\\n\\'html.parser\\'\\n,\\nmulti_valued_attributes\\n=\\nNone\\n)\\nno_list_soup\\n.\\np\\n[\\n\\'class\\'\\n]\\n# \\'body strikeout\\'\\nYou can use\\nget_attribute_list\\nto always return the value in a list\\ncontainer, whether it’s a string or multi-valued attribute value:\\nid_soup\\n.\\np\\n[\\n\\'id\\'\\n]\\n# \\'my id\\'\\nid_soup\\n.\\np\\n.\\nget_attribute_list\\n(\\n\\'id\\'\\n)\\n# [\"my id\"]\\nIf you parse a document as XML, there are no multi-valued attributes:\\nxml_soup\\n=\\nBeautifulSoup\\n(\\n\\'<p class=\"body strikeout\"></p>\\'\\n,\\n\\'xml\\'\\n)\\nxml_soup\\n.\\np\\n[\\n\\'class\\'\\n]\\n# \\'body strikeout\\'\\nAgain, you can configure this using the\\nmulti_valued_attributes\\nargument:\\nclass_is_multi\\n=\\n{\\n\\'*\\'\\n:\\n\\'class\\'\\n}\\nxml_soup\\n=\\nBeautifulSoup\\n(\\n\\'<p class=\"body strikeout\"></p>\\'\\n,\\n\\'xml\\'\\n,\\nmulti_valued_attributes\\n=\\nclass_is_multi\\n)\\nxml_soup\\n.\\np\\n[\\n\\'class\\'\\n]\\n# [\\'body\\', \\'strikeout\\']\\nYou probably won’t need to do this, but if you do, use the defaults as\\na guide. They implement the rules described in the HTML specification:\\nfrom\\nbs4.builder\\nimport\\nbuilder_registry\\nbuilder_registry\\n.\\nlookup\\n(\\n\\'html\\'\\n)\\n.\\nDEFAULT_CDATA_LIST_ATTRIBUTES\\nclass\\nbs4.\\nNavigableString\\n¶\\nA tag can contain strings as pieces of text. Beautiful Soup\\nuses the\\nNavigableString\\nclass to contain these pieces of text:\\nsoup\\n=\\nBeautifulSoup\\n(\\n\\'<b class=\"boldest\">Extremely bold</b>\\'\\n,\\n\\'html.parser\\'\\n)\\ntag\\n=\\nsoup\\n.\\nb\\ntag\\n.\\nstring\\n# \\'Extremely bold\\'\\ntype\\n(\\ntag\\n.\\nstring\\n)\\n# <class \\'bs4.element.NavigableString\\'>\\nA\\nNavigableString\\nis just like a Python Unicode string, except\\nthat it also supports some of the features described in\\nNavigating\\nthe tree\\nand\\nSearching the tree\\n. You can convert a\\nNavigableString\\nto a Unicode string with\\nstr\\n:\\nunicode_string\\n=\\nstr\\n(\\ntag\\n.\\nstring\\n)\\nunicode_string\\n# \\'Extremely bold\\'\\ntype\\n(\\nunicode_string\\n)\\n# <type \\'str\\'>\\nYou can’t edit a string in place, but you can replace one string with\\nanother, using\\nreplace_with()\\n:\\ntag\\n.\\nstring\\n.\\nreplace_with\\n(\\n\"No longer bold\"\\n)\\ntag\\n# <b class=\"boldest\">No longer bold</b>\\nNavigableString\\nsupports most of the features described in\\nNavigating the tree\\nand\\nSearching the tree\\n, but not all of\\nthem. In particular, since a string can’t contain anything (the way a\\ntag may contain a string or another tag), strings don’t support the\\n.contents\\nor\\n.string\\nattributes, or the\\nfind()\\nmethod.\\nIf you want to use a\\nNavigableString\\noutside of Beautiful Soup,\\nyou should call\\nunicode()\\non it to turn it into a normal Python\\nUnicode string. If you don’t, your string will carry around a\\nreference to the entire Beautiful Soup parse tree, even when you’re\\ndone using Beautiful Soup. This is a big waste of memory.\\nclass\\nbs4.\\nBeautifulSoup\\n¶\\nThe\\nBeautifulSoup\\nobject represents the parsed document as a\\nwhole. For most purposes, you can treat it as a\\nTag\\nobject. This means it supports most of the methods described in\\nNavigating the tree\\nand\\nSearching the tree\\n.\\nYou can also pass a\\nBeautifulSoup\\nobject into one of the methods\\ndefined in\\nModifying the tree\\n, just as you would a\\nTag\\n. This\\nlets you do things like combine two parsed documents:\\ndoc\\n=\\nBeautifulSoup\\n(\\n\"<document><content/>INSERT FOOTER HERE</document\"\\n,\\n\"xml\"\\n)\\nfooter\\n=\\nBeautifulSoup\\n(\\n\"<footer>Here\\'s the footer</footer>\"\\n,\\n\"xml\"\\n)\\ndoc\\n.\\nfind\\n(\\ntext\\n=\\n\"INSERT FOOTER HERE\"\\n)\\n.\\nreplace_with\\n(\\nfooter\\n)\\n# \\'INSERT FOOTER HERE\\'\\nprint\\n(\\ndoc\\n)\\n# <?xml version=\"1.0\" encoding=\"utf-8\"?>\\n# <document><content/><footer>Here\\'s the footer</footer></document>\\nSince the\\nBeautifulSoup\\nobject doesn’t correspond to an actual\\nHTML or XML tag, it has no name and no attributes. But sometimes it’s\\nuseful to reference its\\n.name\\n(such as when writing code that works\\nwith both\\nTag\\nand\\nBeautifulSoup\\nobjects),\\nso it’s been given the special\\n.name\\n“[document]”:\\nsoup\\n.\\nname\\n# \\'[document]\\'\\nSpecial strings\\n¶\\nTag\\n,\\nNavigableString\\n, and\\nBeautifulSoup\\ncover almost everything you’ll see in an\\nHTML or XML file, but there are a few leftover bits. The main one\\nyou’ll probably encounter is the\\nComment\\n.\\nclass\\nbs4.\\nComment\\n¶\\nmarkup\\n=\\n\"<b><!--Hey, buddy. Want to buy a used parser?--></b>\"\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n)\\ncomment\\n=\\nsoup\\n.\\nb\\n.\\nstring\\ntype\\n(\\ncomment\\n)\\n# <class \\'bs4.element.Comment\\'>\\nThe\\nComment\\nobject is just a special type of\\nNavigableString\\n:\\ncomment\\n# \\'Hey, buddy. Want to buy a used parser\\'\\nBut when it appears as part of an HTML document, a\\nComment\\nis\\ndisplayed with special formatting:\\nprint\\n(\\nsoup\\n.\\nb\\n.\\nprettify\\n())\\n# <b>\\n# <!--Hey, buddy. Want to buy a used parser?-->\\n# </b>\\nFor HTML documents\\n¶\\nBeautiful Soup defines a few\\nNavigableString\\nsubclasses to\\ncontain strings found inside specific HTML tags. This makes it easier\\nto pick out the main body of the page, by ignoring strings that\\nprobably represent programming directives found within the\\npage.\\n(These classes are new in Beautiful Soup 4.9.0, and the\\nhtml5lib parser doesn’t use them.)\\nclass\\nbs4.\\nStylesheet\\n¶\\nA\\nNavigableString\\nsubclass that represents embedded CSS\\nstylesheets; that is, any strings found inside a\\n<style>\\ntag\\nduring document parsing.\\nclass\\nbs4.\\nScript\\n¶\\nA\\nNavigableString\\nsubclass that represents embedded\\nJavascript; that is, any strings found inside a\\n<script>\\ntag\\nduring document parsing.\\nclass\\nbs4.\\nTemplate\\n¶\\nA\\nNavigableString\\nsubclass that represents embedded HTML\\ntemplates; that is, any strings found inside a\\n<template>\\ntag during\\ndocument parsing.\\nFor XML documents\\n¶\\nBeautiful Soup defines some\\nNavigableString\\nclasses for\\nholding special types of strings that can be found in XML\\ndocuments. Like\\nComment\\n, these classes are subclasses of\\nNavigableString\\nthat add something extra to the string on\\noutput.\\nclass\\nbs4.\\nDeclaration\\n¶\\nA\\nNavigableString\\nsubclass representing the\\ndeclaration\\nat the beginning of\\nan XML document.\\nclass\\nbs4.\\nDoctype\\n¶\\nA\\nNavigableString\\nsubclass representing the\\ndocument type\\ndeclaration\\nwhich may\\nbe found near the beginning of an XML document.\\nclass\\nbs4.\\nCData\\n¶\\nA\\nNavigableString\\nsubclass that represents a\\nCData section\\n.\\nclass\\nbs4.\\nProcessingInstruction\\n¶\\nA\\nNavigableString\\nsubclass that represents the contents\\nof an\\nXML processing instruction\\n.\\nNavigating the tree\\n¶\\nHere’s the “Three sisters” HTML document again:\\nhtml_doc\\n=\\n\"\"\"\\n<html><head><title>The Dormouse\\'s story</title></head>\\n<body>\\n<p class=\"title\"><b>The Dormouse\\'s story</b></p>\\n<p class=\"story\">Once upon a time there were three little sisters; and their names were\\n<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\\n<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\\n<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\\nand they lived at the bottom of a well.</p>\\n<p class=\"story\">...</p>\\n\"\"\"\\nfrom\\nbs4\\nimport\\nBeautifulSoup\\nsoup\\n=\\nBeautifulSoup\\n(\\nhtml_doc\\n,\\n\\'html.parser\\'\\n)\\nI’ll use this as an example to show you how to move from one part of\\na document to another.\\nGoing down\\n¶\\nTags may contain strings and more tags. These elements are the tag’s\\nchildren\\n. Beautiful Soup provides a lot of different attributes for\\nnavigating and iterating over a tag’s children.\\nNote that Beautiful Soup strings don’t support any of these\\nattributes, because a string can’t have children.\\nNavigating using tag names\\n¶\\nThe simplest way to navigate the parse tree is to find a tag by name. To\\ndo this, you can use the\\nfind()\\nmethod:\\nsoup\\n.\\nfind\\n(\\n\"head\"\\n)\\n# <head><title>The Dormouse\\'s story</title></head>\\nFor convenience, just saying the name of the tag you want is equivalent\\nto\\nfind()\\n(if no built-in attribute has that name). If you want the\\n<head> tag, just say\\nsoup.head\\n:\\nsoup\\n.\\nhead\\n# <head><title>The Dormouse\\'s story</title></head>\\nsoup\\n.\\ntitle\\n# <title>The Dormouse\\'s story</title>\\nYou can use this trick again and again to zoom in on a certain part\\nof the parse tree. This code gets the first <b> tag beneath the <body> tag:\\nsoup\\n.\\nbody\\n.\\nb\\n# <b>The Dormouse\\'s story</b>\\nfind()\\n(and its convenience equivalent) gives you only the\\nfirst\\ntag\\nby that name:\\nsoup\\n.\\na\\n# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\\nIf you need to get\\nall\\nthe <a> tags, you can use\\nfind_all()\\n:\\nsoup\\n.\\nfind_all\\n(\\n\\'a\\'\\n)\\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\\n# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\\n# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\\nFor more complicated tasks, such as pattern-matching and filtering, you can\\nuse the methods described in\\nSearching the tree\\n.\\n.contents\\nand\\n.children\\n¶\\nA tag’s children are available in a list called\\n.contents\\n:\\nhead_tag\\n=\\nsoup\\n.\\nhead\\nhead_tag\\n# <head><title>The Dormouse\\'s story</title></head>\\nhead_tag\\n.\\ncontents\\n# [<title>The Dormouse\\'s story</title>]\\ntitle_tag\\n=\\nhead_tag\\n.\\ncontents\\n[\\n0\\n]\\ntitle_tag\\n# <title>The Dormouse\\'s story</title>\\ntitle_tag\\n.\\ncontents\\n# [\\'The Dormouse\\'s story\\']\\nThe\\nBeautifulSoup\\nobject itself has children. In this case, the\\n<html> tag is the child of the\\nBeautifulSoup\\nobject.:\\nlen\\n(\\nsoup\\n.\\ncontents\\n)\\n# 1\\nsoup\\n.\\ncontents\\n[\\n0\\n]\\n.\\nname\\n# \\'html\\'\\nA string does not have\\n.contents\\n, because it can’t contain\\nanything:\\ntext\\n=\\ntitle_tag\\n.\\ncontents\\n[\\n0\\n]\\ntext\\n.\\ncontents\\n# AttributeError: \\'NavigableString\\' object has no attribute \\'contents\\'\\nInstead of getting them as a list, you can iterate over a tag’s\\nchildren using the\\n.children\\ngenerator:\\nfor\\nchild\\nin\\ntitle_tag\\n.\\nchildren\\n:\\nprint\\n(\\nchild\\n)\\n# The Dormouse\\'s story\\nIf you want to modify a tag’s children, use the methods described in\\nModifying the tree\\n. Don’t modify the the\\n.contents\\nlist\\ndirectly: that can lead to problems that are subtle and difficult to\\nspot.\\n.descendants\\n¶\\nThe\\n.contents\\nand\\n.children\\nattributes consider only a tag’s\\ndirect\\nchildren. For instance, the <head> tag has a single direct\\nchild–the <title> tag:\\nhead_tag\\n.\\ncontents\\n# [<title>The Dormouse\\'s story</title>]\\nBut the <title> tag itself has a child: the string “The Dormouse’s\\nstory”. There’s a sense in which that string is also a child of the\\n<head> tag. The\\n.descendants\\nattribute lets you iterate over\\nall\\nof a tag’s children, recursively: its direct children, the children of\\nits direct children, and so on:\\nfor\\nchild\\nin\\nhead_tag\\n.\\ndescendants\\n:\\nprint\\n(\\nchild\\n)\\n# <title>The Dormouse\\'s story</title>\\n# The Dormouse\\'s story\\nThe <head> tag has only one child, but it has two descendants: the\\n<title> tag and the <title> tag’s child. The\\nBeautifulSoup\\nobject\\nonly has one direct child (the <html> tag), but it has a whole lot of\\ndescendants:\\nlen\\n(\\nlist\\n(\\nsoup\\n.\\nchildren\\n))\\n# 1\\nlen\\n(\\nlist\\n(\\nsoup\\n.\\ndescendants\\n))\\n# 26\\n.string\\n¶\\nIf a tag has only one child, and that child is a\\nNavigableString\\n,\\nthe child is made available as\\n.string\\n:\\ntitle_tag\\n.\\nstring\\n# \\'The Dormouse\\'s story\\'\\nIf a tag’s only child is another tag, and\\nthat\\ntag has a\\n.string\\n, then the parent tag is considered to have the same\\n.string\\nas its child:\\nhead_tag\\n.\\ncontents\\n# [<title>The Dormouse\\'s story</title>]\\nhead_tag\\n.\\nstring\\n# \\'The Dormouse\\'s story\\'\\nIf a tag contains more than one thing, then it’s not clear what\\n.string\\nshould refer to, so\\n.string\\nis defined to be\\nNone\\n:\\nprint\\n(\\nsoup\\n.\\nhtml\\n.\\nstring\\n)\\n# None\\n.strings\\nand\\nstripped_strings\\n¶\\nIf there’s more than one thing inside a tag, you can still look at\\njust the strings. Use the\\n.strings\\ngenerator to see all descendant\\nstrings:\\nfor\\nstring\\nin\\nsoup\\n.\\nstrings\\n:\\nprint\\n(\\nrepr\\n(\\nstring\\n))\\n\\'\\n\\\\n\\n\\'\\n# \"The Dormouse\\'s story\"\\n# \\'\\\\n\\'\\n# \\'\\\\n\\'\\n# \"The Dormouse\\'s story\"\\n# \\'\\\\n\\'\\n# \\'Once upon a time there were three little sisters; and their names were\\\\n\\'\\n# \\'Elsie\\'\\n# \\',\\\\n\\'\\n# \\'Lacie\\'\\n# \\' and\\\\n\\'\\n# \\'Tillie\\'\\n# \\';\\\\nand they lived at the bottom of a well.\\'\\n# \\'\\\\n\\'\\n# \\'...\\'\\n# \\'\\\\n\\'\\nNewlines and spaces that separate tags are also strings. You can remove extra\\nwhitespace by using the\\n.stripped_strings\\ngenerator instead:\\nfor\\nstring\\nin\\nsoup\\n.\\nstripped_strings\\n:\\nprint\\n(\\nrepr\\n(\\nstring\\n))\\n# \"The Dormouse\\'s story\"\\n# \"The Dormouse\\'s story\"\\n# \\'Once upon a time there were three little sisters; and their names were\\'\\n# \\'Elsie\\'\\n# \\',\\'\\n# \\'Lacie\\'\\n# \\'and\\'\\n# \\'Tillie\\'\\n# \\';\\\\n and they lived at the bottom of a well.\\'\\n# \\'...\\'\\nHere, strings consisting entirely of whitespace are ignored, and\\nwhitespace at the beginning and end of strings is removed.\\nGoing up\\n¶\\nContinuing the “family tree” analogy, every tag and every string has a\\nparent\\n: the tag that contains it.\\n.parent\\n¶\\nYou can access an element’s parent with the\\n.parent\\nattribute. In\\nthe example “three sisters” document, the <head> tag is the parent\\nof the <title> tag:\\ntitle_tag\\n=\\nsoup\\n.\\ntitle\\ntitle_tag\\n# <title>The Dormouse\\'s story</title>\\ntitle_tag\\n.\\nparent\\n# <head><title>The Dormouse\\'s story</title></head>\\nThe title string itself has a parent: the <title> tag that contains\\nit:\\ntitle_tag\\n.\\nstring\\n.\\nparent\\n# <title>The Dormouse\\'s story</title>\\nThe parent of a top-level tag like <html> is the\\nBeautifulSoup\\nobject\\nitself:\\nhtml_tag\\n=\\nsoup\\n.\\nhtml\\ntype\\n(\\nhtml_tag\\n.\\nparent\\n)\\n# <class \\'bs4.BeautifulSoup\\'>\\nAnd the\\n.parent\\nof a\\nBeautifulSoup\\nobject is defined as None:\\nprint\\n(\\nsoup\\n.\\nparent\\n)\\n# None\\n.parents\\n¶\\nYou can iterate over all of an element’s parents with\\n.parents\\n. This example uses\\n.parents\\nto travel from an <a> tag\\nburied deep within the document, to the very top of the document:\\nlink\\n=\\nsoup\\n.\\na\\nlink\\n# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\\nfor\\nparent\\nin\\nlink\\n.\\nparents\\n:\\nprint\\n(\\nparent\\n.\\nname\\n)\\n# p\\n# body\\n# html\\n# [document]\\nGoing sideways\\n¶\\nConsider a simple document like this:\\nsibling_soup\\n=\\nBeautifulSoup\\n(\\n\"<a><b>text1</b><c>text2</c></a>\"\\n,\\n\\'html.parser\\'\\n)\\nprint\\n(\\nsibling_soup\\n.\\nprettify\\n())\\n# <a>\\n# <b>\\n# text1\\n# </b>\\n# <c>\\n# text2\\n# </c>\\n# </a>\\nThe <b> tag and the <c> tag are at the same level: they’re both direct\\nchildren of the same tag. We call them\\nsiblings\\n. When a document is\\npretty-printed, siblings show up at the same indentation level. You\\ncan also use this relationship in the code you write.\\n.next_sibling\\nand\\n.previous_sibling\\n¶\\nYou can use\\n.next_sibling\\nand\\n.previous_sibling\\nto navigate\\nbetween page elements that are on the same level of the parse tree:\\nsibling_soup\\n.\\nb\\n.\\nnext_sibling\\n# <c>text2</c>\\nsibling_soup\\n.\\nc\\n.\\nprevious_sibling\\n# <b>text1</b>\\nThe <b> tag has a\\n.next_sibling\\n, but no\\n.previous_sibling\\n,\\nbecause there’s nothing before the <b> tag\\non the same level of the\\ntree\\n. For the same reason, the <c> tag has a\\n.previous_sibling\\nbut no\\n.next_sibling\\n:\\nprint\\n(\\nsibling_soup\\n.\\nb\\n.\\nprevious_sibling\\n)\\n# None\\nprint\\n(\\nsibling_soup\\n.\\nc\\n.\\nnext_sibling\\n)\\n# None\\nThe strings “text1” and “text2” are\\nnot\\nsiblings, because they don’t\\nhave the same parent:\\nsibling_soup\\n.\\nb\\n.\\nstring\\n# \\'text1\\'\\nprint\\n(\\nsibling_soup\\n.\\nb\\n.\\nstring\\n.\\nnext_sibling\\n)\\n# None\\nIn real documents, the\\n.next_sibling\\nor\\n.previous_sibling\\nof a\\ntag will usually be a string containing whitespace. Going back to the\\n“three sisters” document:\\n# <a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\\n# <a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\\n# <a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\\nYou might think that the\\n.next_sibling\\nof the first <a> tag would\\nbe the second <a> tag. But actually, it’s a string: the comma and\\nnewline that separate the first <a> tag from the second:\\nlink\\n=\\nsoup\\n.\\na\\nlink\\n# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\\nlink\\n.\\nnext_sibling\\n# \\',\\\\n \\'\\nThe second <a> tag is then the\\n.next_sibling\\nof the comma string:\\nlink\\n.\\nnext_sibling\\n.\\nnext_sibling\\n# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\\n.next_siblings\\nand\\n.previous_siblings\\n¶\\nYou can iterate over a tag’s siblings with\\n.next_siblings\\nor\\n.previous_siblings\\n:\\nfor\\nsibling\\nin\\nsoup\\n.\\na\\n.\\nnext_siblings\\n:\\nprint\\n(\\nrepr\\n(\\nsibling\\n))\\n# \\',\\\\n\\'\\n# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\\n# \\' and\\\\n\\'\\n# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\\n# \\'; and they lived at the bottom of a well.\\'\\nfor\\nsibling\\nin\\nsoup\\n.\\nfind\\n(\\nid\\n=\\n\"link3\"\\n)\\n.\\nprevious_siblings\\n:\\nprint\\n(\\nrepr\\n(\\nsibling\\n))\\n# \\' and\\\\n\\'\\n# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\\n# \\',\\\\n\\'\\n# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\\n# \\'Once upon a time there were three little sisters; and their names were\\\\n\\'\\n(If the argument syntax to find tags by their attribute value is unfamiliar,\\ndon’t worry; this is covered later in\\nThe keyword arguments\\n.)\\nGoing back and forth\\n¶\\nTake a look at the beginning of the “three sisters” document:\\n# <html><head><title>The Dormouse\\'s story</title></head>\\n# <p class=\"title\"><b>The Dormouse\\'s story</b></p>\\nAn HTML parser takes this string of characters and turns it into a\\nseries of events: “open an <html> tag”, “open a <head> tag”, “open a\\n<title> tag”, “add a string”, “close the <title> tag”, “open a <p>\\ntag”, and so on. The order in which the opening tags and strings are\\nencountered is called\\ndocument order\\n. Beautiful Soup offers tools for\\nsearching a document’s elements in document order.\\n.next_element\\nand\\n.previous_element\\n¶\\nThe\\n.next_element\\nattribute of a string or tag points to whatever\\nwas parsed immediately after the opening of the current tag or after\\nthe current string. It might be the same as\\n.next_sibling\\n, but it’s\\nusually drastically different.\\nHere’s the final <a> tag in the “three sisters” document. Its\\n.next_sibling\\nis a string: the conclusion of the sentence that was\\ninterrupted by the start of the <a> tag:\\nlast_a_tag\\n=\\nsoup\\n.\\nfind\\n(\\n\"a\"\\n,\\nid\\n=\\n\"link3\"\\n)\\nlast_a_tag\\n# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\\nlast_a_tag\\n.\\nnext_sibling\\n# \\';\\\\nand they lived at the bottom of a well.\\'\\nBut the\\n.next_element\\nof that <a> tag, the thing that was parsed\\nimmediately after the <a> tag, is\\nnot\\nthe rest of that sentence:\\nit’s the string “Tillie” inside it:\\nlast_a_tag\\n.\\nnext_element\\n# \\'Tillie\\'\\nThat’s because in the original markup, the word “Tillie” appeared\\nbefore that semicolon. The parser encountered an <a> tag, then the\\nword “Tillie”, then the closing </a> tag, then the semicolon and rest of\\nthe sentence. The semicolon is on the same level as the <a> tag, but the\\nword “Tillie” was encountered first.\\nThe\\n.previous_element\\nattribute is the exact opposite of\\n.next_element\\n. It points to the opening tag or string that was\\nparsed immediately before this one:\\nlast_a_tag\\n.\\nprevious_element\\n# \\' and\\\\n\\'\\nlast_a_tag\\n.\\nprevious_element\\n.\\nnext_element\\n# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\\n.next_elements\\nand\\n.previous_elements\\n¶\\nYou should get the idea by now. You can use these iterators to move\\nforward or backward in the document as it was parsed:\\nfor\\nelement\\nin\\nlast_a_tag\\n.\\nnext_elements\\n:\\nprint\\n(\\nrepr\\n(\\nelement\\n))\\n# \\'Tillie\\'\\n# \\';\\\\nand they lived at the bottom of a well.\\'\\n# \\'\\\\n\\'\\n# <p class=\"story\">...</p>\\n# \\'...\\'\\n# \\'\\\\n\\'\\nSearching the tree\\n¶\\nBeautiful Soup defines a lot of methods for searching the parse tree,\\nbut they’re all very similar. I’m going to spend a lot of time explaining\\nthe two most popular methods:\\nfind()\\nand\\nfind_all()\\n. The other\\nmethods take almost exactly the same arguments, so I’ll just cover\\nthem briefly.\\nOnce again, I’ll be using the “three sisters” document as an example:\\nhtml_doc\\n=\\n\"\"\"\\n<html><head><title>The Dormouse\\'s story</title></head>\\n<body>\\n<p class=\"title\"><b>The Dormouse\\'s story</b></p>\\n<p class=\"story\">Once upon a time there were three little sisters; and their names were\\n<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\\n<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\\n<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\\nand they lived at the bottom of a well.</p>\\n<p class=\"story\">...</p>\\n\"\"\"\\nfrom\\nbs4\\nimport\\nBeautifulSoup\\nsoup\\n=\\nBeautifulSoup\\n(\\nhtml_doc\\n,\\n\\'html.parser\\'\\n)\\nBy passing in a filter to a method like\\nfind_all()\\n, you can\\nzoom in on the parts of the document you’re interested in.\\nKinds of filters\\n¶\\nBefore talking in detail about\\nfind_all()\\nand similar methods, I\\nwant to show examples of different filters you can pass into these\\nmethods. These filters show up again and again, throughout the\\nsearch API. You can use them to filter based on a tag’s name,\\non its attributes, on the text of a string, or on some combination of\\nthese.\\nA string\\n¶\\nThe simplest filter is a string. Pass a string to a search method and\\nBeautiful Soup will perform a tag-name match against that exact string.\\nThis code finds all the <b> tags in the document:\\nsoup\\n.\\nfind_all\\n(\\n\\'b\\'\\n)\\n# [<b>The Dormouse\\'s story</b>]\\nIf you pass in a byte string, Beautiful Soup will assume the string is\\nencoded as UTF-8. You can avoid this by passing in a Unicode string instead.\\nA regular expression\\n¶\\nIf you pass in a regular expression object, Beautiful Soup will filter\\nagainst that regular expression using its\\nsearch()\\nmethod. This code\\nfinds all the tags whose names start with the letter “b”; in this\\ncase, the <body> tag and the <b> tag:\\nimport\\nre\\nfor\\ntag\\nin\\nsoup\\n.\\nfind_all\\n(\\nre\\n.\\ncompile\\n(\\n\"^b\"\\n)):\\nprint\\n(\\ntag\\n.\\nname\\n)\\n# body\\n# b\\nThis code finds all the tags whose names contain the letter ‘t’:\\nfor\\ntag\\nin\\nsoup\\n.\\nfind_all\\n(\\nre\\n.\\ncompile\\n(\\n\"t\"\\n)):\\nprint\\n(\\ntag\\n.\\nname\\n)\\n# html\\n# title\\nTrue\\n¶\\nThe value\\nTrue\\nmatches every tag it can. This code finds\\nall\\nthe tags in the document, but none of the text strings:\\nfor\\ntag\\nin\\nsoup\\n.\\nfind_all\\n(\\nTrue\\n):\\nprint\\n(\\ntag\\n.\\nname\\n)\\n# html\\n# head\\n# title\\n# body\\n# p\\n# b\\n# p\\n# a\\n# a\\n# a\\n# p\\nA function\\n¶\\nIf none of the other matches work for you, define a function that\\ntakes an element as its only argument. The function should return\\nTrue\\nif the argument matches, and\\nFalse\\notherwise.\\nHere’s a function that returns\\nTrue\\nif a tag defines the “class”\\nattribute but doesn’t define the “id” attribute:\\ndef\\nhas_class_but_no_id\\n(\\ntag\\n):\\nreturn\\ntag\\n.\\nhas_attr\\n(\\n\\'class\\'\\n)\\nand\\nnot\\ntag\\n.\\nhas_attr\\n(\\n\\'id\\'\\n)\\nPass this function into\\nfind_all()\\nand you’ll pick up all the <p>\\ntags:\\nsoup\\n.\\nfind_all\\n(\\nhas_class_but_no_id\\n)\\n# [<p class=\"title\"><b>The Dormouse\\'s story</b></p>,\\n# <p class=\"story\">Once upon a time there were…bottom of a well.</p>,\\n# <p class=\"story\">...</p>]\\nThis function picks up only the <p> tags. It doesn’t pick up the <a>\\ntags, because those tags define both “class” and “id”. It doesn’t pick\\nup tags like <html> and <title>, because those tags don’t define\\n“class”.\\nThe function can be as complicated as you need it to be. Here’s a\\nfunction that returns\\nTrue\\nif a tag is surrounded by string\\nobjects:\\nfrom\\nbs4\\nimport\\nNavigableString\\ndef\\nsurrounded_by_strings\\n(\\ntag\\n):\\nreturn\\n(\\nisinstance\\n(\\ntag\\n.\\nnext_element\\n,\\nNavigableString\\n)\\nand\\nisinstance\\n(\\ntag\\n.\\nprevious_element\\n,\\nNavigableString\\n))\\nfor\\ntag\\nin\\nsoup\\n.\\nfind_all\\n(\\nsurrounded_by_strings\\n):\\nprint\\n(\\ntag\\n.\\nname\\n)\\n# body\\n# p\\n# a\\n# a\\n# a\\n# p\\nA list\\n¶\\nIf you pass in a list, Beautiful Soup will look for a match against\\nany\\nstring, regular expression, or function in that list. This\\ncode finds all the <a> tags\\nand\\nall the <b> tags:\\nsoup\\n.\\nfind_all\\n([\\n\"a\"\\n,\\n\"b\"\\n])\\n# [<b>The Dormouse\\'s story</b>,\\n# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\\n# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\\n# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\\nNow we’re ready to look at the search methods in detail.\\nfind_all()\\n¶\\nMethod signature: find_all(\\nname\\n,\\nattrs\\n,\\nrecursive\\n,\\nstring\\n,\\nlimit\\n,\\n**kwargs\\n)\\nThe\\nfind_all()\\nmethod looks through a tag’s descendants and\\nretrieves\\nall\\ndescendants that match your filters. I gave several\\nexamples in\\nKinds of filters\\n, but here are a few more:\\nsoup\\n.\\nfind_all\\n(\\n\"title\"\\n)\\n# [<title>The Dormouse\\'s story</title>]\\nsoup\\n.\\nfind_all\\n(\\n\"p\"\\n,\\n\"title\"\\n)\\n# [<p class=\"title\"><b>The Dormouse\\'s story</b></p>]\\nsoup\\n.\\nfind_all\\n(\\n\"a\"\\n)\\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\\n# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\\n# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\\nsoup\\n.\\nfind_all\\n(\\nid\\n=\\n\"link2\"\\n)\\n# [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]\\nimport\\nre\\nsoup\\n.\\nfind\\n(\\nstring\\n=\\nre\\n.\\ncompile\\n(\\n\"sisters\"\\n))\\n# \\'Once upon a time there were three little sisters; and their names were\\\\n\\'\\nSome of these should look familiar, but others are new. What does it\\nmean to pass in a value for\\nstring\\n, or\\nid\\n? Why does\\nfind_all(\"p\",\\n\"title\")\\nfind a <p> tag with the CSS class “title”?\\nLet’s look at the arguments to\\nfind_all()\\n.\\nThe\\nname\\nargument\\n¶\\nPass in a value for\\nname\\nand you’ll tell Beautiful Soup to only\\nconsider tags with certain names. Text strings will be ignored, as\\nwill tags whose names that don’t match.\\nThis is the simplest usage:\\nsoup\\n.\\nfind_all\\n(\\n\"title\"\\n)\\n# [<title>The Dormouse\\'s story</title>]\\nRecall from\\nKinds of filters\\nthat the value to\\nname\\ncan be\\na\\nstring\\n,\\na regular expression\\n,\\na list\\n,\\na function\\n, or\\nthe value\\nTrue\\n.\\nThe keyword arguments\\n¶\\nAny keyword argument that’s not recognized will be turned into a filter\\nthat matches tags by their attributes.\\nIf you pass in a value for an argument called\\nid\\n, Beautiful Soup will\\nfilter against each tag’s ‘id’ attribute value:\\nsoup\\n.\\nfind_all\\n(\\nid\\n=\\n\\'link2\\'\\n)\\n# [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]\\nJust as with tags, you can filter an attribute based on\\na string\\n,\\na regular expression\\n,\\na list\\n,\\na function\\n, or\\nthe value True\\n.\\nIf you pass in a regular expression object for\\nhref\\n, Beautiful Soup will\\npattern-match against each tag’s ‘href’ attribute value:\\nsoup\\n.\\nfind_all\\n(\\nhref\\n=\\nre\\n.\\ncompile\\n(\\n\"elsie\"\\n))\\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]\\nThe value\\nTrue\\nmatches every tag that defines the attribute. This code\\nfinds\\nall\\ntags with an\\nid\\nattribute:\\nsoup.find_all(id=True)\\n# [<a class=”sister” href=”\\nhttp://example.com/elsie\\n” id=”link1”>Elsie</a>,\\n# <a class=”sister” href=”\\nhttp://example.com/lacie\\n” id=”link2”>Lacie</a>,\\n# <a class=”sister” href=”\\nhttp://example.com/tillie\\n” id=”link3”>Tillie</a>]\\nFor more complex matches, you can define a function that takes an attribute\\nvalue as its only argument. The function should return\\nTrue\\nif the value\\nmatches, and\\nFalse\\notherwise.\\nHere’s a function that finds all\\na\\ntags whose\\nhref\\nattribute\\ndoes not\\nmatch a regular expression:\\nimport\\nre\\ndef\\nnot_lacie\\n(\\nhref\\n):\\nreturn\\nhref\\nand\\nnot\\nre\\n.\\ncompile\\n(\\n\"lacie\"\\n)\\n.\\nsearch\\n(\\nhref\\n)\\nsoup\\n.\\nfind_all\\n(\\nhref\\n=\\nnot_lacie\\n)\\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\\n# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\\nIf you pass in a list for an argument, Beautiful Soup will look for an\\nattribute-value match against\\nany\\nstring, regular expression, or function in\\nthat list. This code finds the first and last link:\\nsoup.find_all(id=[“link1”, re.compile(“3$”)])\\n# [<a class=”sister” href=”\\nhttp://example.com/elsie\\n” id=”link1”>Elsie</a>,\\n# <a class=”sister” href=”\\nhttp://example.com/tillie\\n” id=”link3”>Tillie</a>]\\nYou can filter against multiple attributes at once by passing multiple\\nkeyword arguments:\\nsoup\\n.\\nfind_all\\n(\\nhref\\n=\\nre\\n.\\ncompile\\n(\\n\"elsie\"\\n),\\nid\\n=\\n\\'link1\\'\\n)\\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]\\nSome attributes, like the data-* attributes in HTML 5, have names that\\ncan’t be used as the names of keyword arguments:\\ndata_soup\\n=\\nBeautifulSoup\\n(\\n\\'<div data-foo=\"value\">foo!</div>\\'\\n,\\n\\'html.parser\\'\\n)\\ndata_soup\\n.\\nfind_all\\n(\\ndata\\n-\\nfoo\\n=\\n\"value\"\\n)\\n# SyntaxError: keyword can\\'t be an expression\\nYou can use these attributes in searches by putting them into a\\ndictionary and passing the dictionary into\\nfind_all()\\nas the\\nattrs\\nargument:\\ndata_soup\\n.\\nfind_all\\n(\\nattrs\\n=\\n{\\n\"data-foo\"\\n:\\n\"value\"\\n})\\n# [<div data-foo=\"value\">foo!</div>]\\nSimilarly, you can’t use a keyword argument to search for HTML’s ‘name’ attribute,\\nbecause Beautiful Soup uses the\\nname\\nargument to contain the name\\nof the tag itself. Instead, you can give a value to ‘name’ in the\\nattrs\\nargument:\\nname_soup\\n=\\nBeautifulSoup\\n(\\n\\'<input name=\"email\"/>\\'\\n,\\n\\'html.parser\\'\\n)\\nname_soup\\n.\\nfind_all\\n(\\nname\\n=\\n\"email\"\\n)\\n# []\\nname_soup\\n.\\nfind_all\\n(\\nattrs\\n=\\n{\\n\"name\"\\n:\\n\"email\"\\n})\\n# [<input name=\"email\"/>]\\nSearching by CSS class\\n¶\\nIt’s very useful to search for a tag that has a certain CSS class, but\\nthe name of the CSS attribute, “class”, is a reserved word in\\nPython. Using\\nclass\\nas a keyword argument will give you a syntax\\nerror. As of Beautiful Soup 4.1.2, you can search by CSS class using\\nthe keyword argument\\nclass_\\n:\\nsoup\\n.\\nfind_all\\n(\\n\"a\"\\n,\\nclass_\\n=\\n\"sister\"\\n)\\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\\n# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\\n# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\\nAs with any keyword argument, you can pass\\nclass_\\na string, a regular\\nexpression, a function, or\\nTrue\\n:\\nsoup\\n.\\nfind_all\\n(\\nclass_\\n=\\nre\\n.\\ncompile\\n(\\n\"itl\"\\n))\\n# [<p class=\"title\"><b>The Dormouse\\'s story</b></p>]\\ndef\\nhas_six_characters\\n(\\ncss_class\\n):\\nreturn\\ncss_class\\nis\\nnot\\nNone\\nand\\nlen\\n(\\ncss_class\\n)\\n==\\n6\\nsoup\\n.\\nfind_all\\n(\\nclass_\\n=\\nhas_six_characters\\n)\\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\\n# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\\n# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\\nRemember\\nthat a single tag can have multiple\\nvalues for its “class” attribute. When you search for a tag that\\nmatches a certain CSS class, you’re matching against\\nany\\nof its CSS\\nclasses:\\ncss_soup\\n=\\nBeautifulSoup\\n(\\n\\'<p class=\"body strikeout\"></p>\\'\\n,\\n\\'html.parser\\'\\n)\\ncss_soup\\n.\\nfind_all\\n(\\n\"p\"\\n,\\nclass_\\n=\\n\"strikeout\"\\n)\\n# [<p class=\"body strikeout\"></p>]\\ncss_soup\\n.\\nfind_all\\n(\\n\"p\"\\n,\\nclass_\\n=\\n\"body\"\\n)\\n# [<p class=\"body strikeout\"></p>]\\nYou can also search for the exact string value of the\\nclass\\nattribute:\\ncss_soup\\n.\\nfind_all\\n(\\n\"p\"\\n,\\nclass_\\n=\\n\"body strikeout\"\\n)\\n# [<p class=\"body strikeout\"></p>]\\nBut searching for variants of the string value won’t work:\\ncss_soup\\n.\\nfind_all\\n(\\n\"p\"\\n,\\nclass_\\n=\\n\"strikeout body\"\\n)\\n# []\\nIn older versions of Beautiful Soup, which don’t have the\\nclass_\\nshortcut, you can use the\\nattrs\\nargument trick mentioned above.\\nCreate a dictionary whose value for “class” is the string (or regular\\nexpression, or whatever) you want to search for:\\nsoup\\n.\\nfind_all\\n(\\n\"a\"\\n,\\nattrs\\n=\\n{\\n\"class\"\\n:\\n\"sister\"\\n})\\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\\n# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\\n# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\\nTo search for tags that match two or more CSS classes at once, use the\\nselect()\\nCSS selector method\\ndescribed here\\n:\\ncss_soup\\n.\\nselect\\n(\\n\"p.strikeout.body\"\\n)\\n# [<p class=\"body strikeout\"></p>]\\nThe\\nstring\\nargument\\n¶\\nWith the\\nstring\\nargument, you can search for strings instead of tags. As\\nwith\\nname\\nand attribute keyword arguments, you can pass in\\na string\\n,\\na\\nregular expression\\n,\\na function\\n,\\na list\\n, or\\nthe value True\\n.\\nHere are some examples:\\nsoup\\n.\\nfind_all\\n(\\nstring\\n=\\n\"Elsie\"\\n)\\n# [\\'Elsie\\']\\nsoup\\n.\\nfind_all\\n(\\nstring\\n=\\n[\\n\"Tillie\"\\n,\\n\"Elsie\"\\n,\\n\"Lacie\"\\n])\\n# [\\'Elsie\\', \\'Lacie\\', \\'Tillie\\']\\nsoup\\n.\\nfind_all\\n(\\nstring\\n=\\nre\\n.\\ncompile\\n(\\n\"Dormouse\"\\n))\\n# [\"The Dormouse\\'s story\", \"The Dormouse\\'s story\"]\\ndef\\nis_the_only_string_within_a_tag\\n(\\ns\\n):\\n\"\"\"Return True if this string is the only child of its parent tag.\"\"\"\\nreturn\\n(\\ns\\n==\\ns\\n.\\nparent\\n.\\nstring\\n)\\nsoup\\n.\\nfind_all\\n(\\nstring\\n=\\nis_the_only_string_within_a_tag\\n)\\n# [\"The Dormouse\\'s story\", \"The Dormouse\\'s story\", \\'Elsie\\', \\'Lacie\\', \\'Tillie\\', \\'...\\']\\nIf you use the\\nstring\\nargument in a tag search, Beautiful Soup will find\\nall tags whose\\n.string\\nmatches your value for\\nstring\\n. This code finds\\nthe <a> tags whose\\n.string\\nis “Elsie”:\\nsoup\\n.\\nfind_all\\n(\\n\"a\"\\n,\\nstring\\n=\\n\"Elsie\"\\n)\\n# [<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>]\\nThe\\nstring\\nargument is new in Beautiful Soup 4.4.0. In earlier\\nversions it was called\\ntext\\n:\\nsoup\\n.\\nfind_all\\n(\\n\"a\"\\n,\\ntext\\n=\\n\"Elsie\"\\n)\\n# [<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>]\\nThe\\nlimit\\nargument\\n¶\\nfind_all()\\nreturns all the tags and strings that match your\\nfilters. This can take a while if the document is large. If you don’t\\nneed\\nall\\nthe results, you can pass in a number for\\nlimit\\n. This\\nworks just like the LIMIT keyword in SQL. It tells Beautiful Soup to\\nstop gathering results after it’s found a certain number.\\nThere are three links in the “three sisters” document, but this code\\nonly finds the first two:\\nsoup\\n.\\nfind_all\\n(\\n\"a\"\\n,\\nlimit\\n=\\n2\\n)\\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\\n# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]\\nThe\\nrecursive\\nargument\\n¶\\nBy default,\\nmytag.find_all()\\nwill examine all the descendants of\\nmytag\\n:\\nits children, its children’s children, and so on. To consider only direct\\nchildren, you can pass in\\nrecursive=False\\n. See the difference here:\\nsoup\\n.\\nhtml\\n.\\nfind_all\\n(\\n\"title\"\\n)\\n# [<title>The Dormouse\\'s story</title>]\\nsoup\\n.\\nhtml\\n.\\nfind_all\\n(\\n\"title\"\\n,\\nrecursive\\n=\\nFalse\\n)\\n# []\\nHere’s that part of the document:\\n<\\nhtml\\n>\\n<\\nhead\\n>\\n<\\ntitle\\n>\\nThe\\nDormouse\\n\\'s story\\n</\\ntitle\\n>\\n</\\nhead\\n>\\n...\\nThe <title> tag is beneath the <html> tag, but it’s not\\ndirectly\\nbeneath the <html> tag: the <head> tag is in the way. Beautiful Soup\\nfinds the <title> tag when it’s allowed to look at all descendants of\\nthe <html> tag, but when\\nrecursive=False\\nrestricts it to the\\n<html> tag’s immediate children, it finds nothing.\\nBeautiful Soup offers a lot of tree-searching methods (covered below),\\nand they mostly take the same arguments as\\nfind_all()\\n:\\nname\\n,\\nattrs\\n,\\nstring\\n,\\nlimit\\n, and attribute keyword arguments. But the\\nrecursive\\nargument is specific to the\\nfind_all()\\nand\\nfind()\\nmethods.\\nPassing\\nrecursive=False\\ninto a method like\\nfind_parents()\\nwouldn’t be\\nvery useful.\\nCalling a tag is like calling\\nfind_all()\\n¶\\nFor convenience, calling a\\nBeautifulSoup\\nobject or\\nTag\\nobject as a function is equivalent to calling\\nfind_all()\\n(if no built-in method has the name of the tag you’re\\nlooking for). These two lines of code are equivalent:\\nsoup\\n.\\nfind_all\\n(\\n\"a\"\\n)\\nsoup\\n(\\n\"a\"\\n)\\nThese two lines are also equivalent:\\nsoup\\n.\\ntitle\\n.\\nfind_all\\n(\\nstring\\n=\\nTrue\\n)\\nsoup\\n.\\ntitle\\n(\\nstring\\n=\\nTrue\\n)\\nfind()\\n¶\\nMethod signature: find(\\nname\\n,\\nattrs\\n,\\nrecursive\\n,\\nstring\\n,\\n**kwargs\\n)\\nThe\\nfind_all()\\nmethod scans the entire document looking for\\nresults, but sometimes you only want to find one result. If you know a\\ndocument has only one <body> tag, it’s a waste of time to scan the\\nentire document looking for more. Rather than passing in\\nlimit=1\\nevery time you call\\nfind_all\\n, you can use the\\nfind()\\nmethod. These two lines of code are\\nnearly\\nequivalent:\\nsoup\\n.\\nfind_all\\n(\\n\\'title\\'\\n,\\nlimit\\n=\\n1\\n)\\n# [<title>The Dormouse\\'s story</title>]\\nsoup\\n.\\nfind\\n(\\n\\'title\\'\\n)\\n# <title>The Dormouse\\'s story</title>\\nThe only difference is that\\nfind_all()\\nreturns a list containing\\nthe single result, and\\nfind()\\njust returns the result.\\nIf\\nfind_all()\\ncan’t find anything, it returns an empty list. If\\nfind()\\ncan’t find anything, it returns\\nNone\\n:\\nprint\\n(\\nsoup\\n.\\nfind\\n(\\n\"nosuchtag\"\\n))\\n# None\\nRemember the\\nsoup.head.title\\ntrick from\\nNavigating using tag\\nnames\\n? That trick works by repeatedly calling\\nfind()\\n:\\nsoup\\n.\\nhead\\n.\\ntitle\\n# <title>The Dormouse\\'s story</title>\\nsoup\\n.\\nfind\\n(\\n\"head\"\\n)\\n.\\nfind\\n(\\n\"title\"\\n)\\n# <title>The Dormouse\\'s story</title>\\nfind_parents()\\nand\\nfind_parent()\\n¶\\nMethod signature: find_parents(\\nname\\n,\\nattrs\\n,\\nstring\\n,\\nlimit\\n,\\n**kwargs\\n)\\nMethod signature: find_parent(\\nname\\n,\\nattrs\\n,\\nstring\\n,\\n**kwargs\\n)\\nI spent a lot of time above covering\\nfind_all()\\nand\\nfind()\\n. The Beautiful Soup API defines ten other methods for\\nsearching the tree, but don’t be afraid. Five of these methods are\\nbasically the same as\\nfind_all()\\n, and the other five are basically\\nthe same as\\nfind()\\n. The only differences are in how they move from\\none part of the tree to another.\\nFirst let’s consider\\nfind_parents()\\nand\\nfind_parent()\\n. Remember that\\nfind_all()\\nand\\nfind()\\nwork\\ntheir way down the tree, looking at tag’s descendants. These methods\\ndo the opposite: they work their way\\nup\\nthe tree, looking at a tag’s\\n(or a string’s) parents. Let’s try them out, starting from a string\\nburied deep in the “three daughters” document:\\na_string\\n=\\nsoup\\n.\\nfind\\n(\\nstring\\n=\\n\"Lacie\"\\n)\\na_string\\n# \\'Lacie\\'\\na_string\\n.\\nfind_parents\\n(\\n\"a\"\\n)\\n# [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]\\na_string\\n.\\nfind_parent\\n(\\n\"p\"\\n)\\n# <p class=\"story\">Once upon a time there were three little sisters; and their names were\\n# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\\n# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\\n# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\\n# and they lived at the bottom of a well.</p>\\na_string\\n.\\nfind_parents\\n(\\n\"p\"\\n,\\nclass_\\n=\\n\"title\"\\n)\\n# []\\nOne of the three <a> tags is the direct parent of the string in\\nquestion, so our search finds it. One of the three <p> tags is an\\nindirect parent (\\nancestor\\n) of the string, and our search finds that as\\nwell. There’s a <p> tag with the CSS class “title”\\nsomewhere\\nin the\\ndocument, but it’s not one of this string’s parents, so we can’t find\\nit with\\nfind_parents()\\n.\\nYou may have noticed a similarity between\\nfind_parent()\\nand\\nfind_parents()\\n, and the\\n.parent\\nand\\n.parents\\nattributes\\nmentioned earlier. These search methods actually use the\\n.parents\\nattribute to iterate through all parents (unfiltered), checking each one\\nagainst the provided filter to see if it matches.\\nfind_next_siblings()\\nand\\nfind_next_sibling()\\n¶\\nMethod signature: find_next_siblings(\\nname\\n,\\nattrs\\n,\\nstring\\n,\\nlimit\\n,\\n**kwargs\\n)\\nMethod signature: find_next_sibling(\\nname\\n,\\nattrs\\n,\\nstring\\n,\\n**kwargs\\n)\\nThese methods use\\n.next_siblings\\nto\\niterate over the rest of an element’s siblings in the tree. The\\nfind_next_siblings()\\nmethod returns all the siblings that match,\\nand\\nfind_next_sibling()\\nreturns only the first one:\\nfirst_link\\n=\\nsoup\\n.\\na\\nfirst_link\\n# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\\nfirst_link\\n.\\nfind_next_siblings\\n(\\n\"a\"\\n)\\n# [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\\n# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\\nfirst_story_paragraph\\n=\\nsoup\\n.\\nfind\\n(\\n\"p\"\\n,\\n\"story\"\\n)\\nfirst_story_paragraph\\n.\\nfind_next_sibling\\n(\\n\"p\"\\n)\\n# <p class=\"story\">...</p>\\nfind_previous_siblings()\\nand\\nfind_previous_sibling()\\n¶\\nMethod signature: find_previous_siblings(\\nname\\n,\\nattrs\\n,\\nstring\\n,\\nlimit\\n,\\n**kwargs\\n)\\nMethod signature: find_previous_sibling(\\nname\\n,\\nattrs\\n,\\nstring\\n,\\n**kwargs\\n)\\nThese methods use\\n.previous_siblings\\nto iterate over an element’s\\nsiblings that precede it in the tree. The\\nfind_previous_siblings()\\nmethod returns all the siblings that match, and\\nfind_previous_sibling()\\nreturns only the first one:\\nlast_link\\n=\\nsoup\\n.\\nfind\\n(\\n\"a\"\\n,\\nid\\n=\\n\"link3\"\\n)\\nlast_link\\n# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\\nlast_link\\n.\\nfind_previous_siblings\\n(\\n\"a\"\\n)\\n# [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\\n# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]\\nfirst_story_paragraph\\n=\\nsoup\\n.\\nfind\\n(\\n\"p\"\\n,\\n\"story\"\\n)\\nfirst_story_paragraph\\n.\\nfind_previous_sibling\\n(\\n\"p\"\\n)\\n# <p class=\"title\"><b>The Dormouse\\'s story</b></p>\\nfind_all_next()\\nand\\nfind_next()\\n¶\\nMethod signature: find_all_next(\\nname\\n,\\nattrs\\n,\\nstring\\n,\\nlimit\\n,\\n**kwargs\\n)\\nMethod signature: find_next(\\nname\\n,\\nattrs\\n,\\nstring\\n,\\n**kwargs\\n)\\nThese methods use\\n.next_elements\\nto\\niterate over whatever tags and strings that come after it in the\\ndocument. The\\nfind_all_next()\\nmethod returns all matches, and\\nfind_next()\\nreturns only the first match:\\nfirst_link\\n=\\nsoup\\n.\\na\\nfirst_link\\n# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\\nfirst_link\\n.\\nfind_all_next\\n(\\nstring\\n=\\nTrue\\n)\\n# [\\'Elsie\\', \\',\\\\n\\', \\'Lacie\\', \\' and\\\\n\\', \\'Tillie\\',\\n# \\';\\\\nand they lived at the bottom of a well.\\', \\'\\\\n\\', \\'...\\', \\'\\\\n\\']\\nfirst_link\\n.\\nfind_next\\n(\\n\"p\"\\n)\\n# <p class=\"story\">...</p>\\nIn the first example, the string “Elsie” showed up, even though it was\\ncontained within the <a> tag we started from. In the second example,\\nthe last <p> tag in the document showed up, even though it’s not in\\nthe same part of the tree as the <a> tag we started from. For these\\nmethods, all that matters is that an element matches the filter and\\nit shows up later in the document in\\ndocument order\\n.\\nfind_all_previous()\\nand\\nfind_previous()\\n¶\\nMethod signature: find_all_previous(\\nname\\n,\\nattrs\\n,\\nstring\\n,\\nlimit\\n,\\n**kwargs\\n)\\nMethod signature: find_previous(\\nname\\n,\\nattrs\\n,\\nstring\\n,\\n**kwargs\\n)\\nThese methods use\\n.previous_elements\\nto\\niterate over the tags and strings that came before it in the\\ndocument. The\\nfind_all_previous()\\nmethod returns all matches, and\\nfind_previous()\\nonly returns the first match:\\nfirst_link\\n=\\nsoup\\n.\\na\\nfirst_link\\n# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\\nfirst_link\\n.\\nfind_all_previous\\n(\\n\"p\"\\n)\\n# [<p class=\"story\">Once upon a time there were three little sisters; ...</p>,\\n# <p class=\"title\"><b>The Dormouse\\'s story</b></p>]\\nfirst_link\\n.\\nfind_previous\\n(\\n\"title\"\\n)\\n# <title>The Dormouse\\'s story</title>\\nThe call to\\nfind_all_previous(\"p\")\\nfound the first paragraph in\\nthe document (the one with class=”title”), but it also finds the\\nsecond paragraph, the <p> tag that contains the <a> tag we started\\nwith. This shouldn’t be too surprising: we’re looking at all the tags\\nthat show up earlier in the document in\\ndocument order\\nthan the one we started with. A\\n<p> tag that contains an <a> tag must have shown up before the <a>\\ntag it contains.\\nCSS selectors through the\\n.css\\nproperty\\n¶\\nBeautifulSoup\\nand\\nTag\\nobjects support CSS selectors through\\ntheir\\n.css\\nproperty. The actual selector implementation is handled\\nby the\\nSoup Sieve\\npackage, available on PyPI as\\nsoupsieve\\n. If you installed\\nBeautiful Soup through\\npip\\n, Soup Sieve was installed at the same\\ntime, so you don’t have to do anything extra.\\nThe Soup Sieve documentation lists\\nall the currently supported CSS\\nselectors\\n, but\\nhere are some of the basics. You can find tags by name:\\nsoup\\n.\\ncss\\n.\\nselect\\n(\\n\"title\"\\n)\\n# [<title>The Dormouse\\'s story</title>]\\nsoup\\n.\\ncss\\n.\\nselect\\n(\\n\"p:nth-of-type(3)\"\\n)\\n# [<p class=\"story\">...</p>]\\nFind tags by ID:\\nsoup\\n.\\ncss\\n.\\nselect\\n(\\n\"#link1\"\\n)\\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]\\nsoup\\n.\\ncss\\n.\\nselect\\n(\\n\"a#link2\"\\n)\\n# [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]\\nFind tags contained anywhere within other tags:\\nsoup\\n.\\ncss\\n.\\nselect\\n(\\n\"body a\"\\n)\\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\\n# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\\n# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\\nsoup\\n.\\ncss\\n.\\nselect\\n(\\n\"html head title\"\\n)\\n# [<title>The Dormouse\\'s story</title>]\\nFind tags\\ndirectly\\nwithin other tags:\\nsoup\\n.\\ncss\\n.\\nselect\\n(\\n\"head > title\"\\n)\\n# [<title>The Dormouse\\'s story</title>]\\nsoup\\n.\\ncss\\n.\\nselect\\n(\\n\"p > a\"\\n)\\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\\n# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\\n# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\\nsoup\\n.\\ncss\\n.\\nselect\\n(\\n\"p > a:nth-of-type(2)\"\\n)\\n# [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]\\nsoup\\n.\\ncss\\n.\\nselect\\n(\\n\"body > a\"\\n)\\n# []\\nFind all matching next siblings of tags:\\nsoup\\n.\\ncss\\n.\\nselect\\n(\\n\"#link1 ~ .sister\"\\n)\\n# [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\\n# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\\nFind the next sibling tag (but only if it matches):\\nsoup\\n.\\ncss\\n.\\nselect\\n(\\n\"#link1 + .sister\"\\n)\\n# [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]\\nFind tags by CSS class:\\nsoup\\n.\\ncss\\n.\\nselect\\n(\\n\".sister\"\\n)\\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\\n# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\\n# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\\nsoup\\n.\\ncss\\n.\\nselect\\n(\\n\"[class~=sister]\"\\n)\\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\\n# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\\n# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\\nFind tags that match any selector from a list of selectors:\\nsoup\\n.\\ncss\\n.\\nselect\\n(\\n\"#link1,#link2\"\\n)\\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\\n# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]\\nTest for the existence of an attribute:\\nsoup\\n.\\ncss\\n.\\nselect\\n(\\n\\'a[href]\\'\\n)\\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\\n# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\\n# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\\nFind tags by attribute value:\\nsoup\\n.\\ncss\\n.\\nselect\\n(\\n\\'a[href=\"http://example.com/elsie\"]\\'\\n)\\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]\\nsoup\\n.\\ncss\\n.\\nselect\\n(\\n\\'a[href^=\"http://example.com/\"]\\'\\n)\\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\\n# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\\n# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\\nsoup\\n.\\ncss\\n.\\nselect\\n(\\n\\'a[href$=\"tillie\"]\\'\\n)\\n# [<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\\nsoup\\n.\\ncss\\n.\\nselect\\n(\\n\\'a[href*=\".com/el\"]\\'\\n)\\n# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]\\nThere’s also a method called\\nselect_one()\\n, which finds only the\\nfirst tag that matches a selector:\\nsoup\\n.\\ncss\\n.\\nselect_one\\n(\\n\".sister\"\\n)\\n# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\\nAs a convenience, you can call\\nselect()\\nand\\nselect_one()\\ncan\\ndirectly on the\\nBeautifulSoup\\nor\\nTag\\nobject, omitting the\\n.css\\nproperty:\\nsoup\\n.\\nselect\\n(\\n\\'a[href$=\"tillie\"]\\'\\n)\\n# [<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\\nsoup\\n.\\nselect_one\\n(\\n\".sister\"\\n)\\n# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\\nCSS selector support is a convenience for people who already know the\\nCSS selector syntax. You can do all of this with the Beautiful Soup\\nAPI. If CSS selectors are all you need, you should skip Beautiful Soup\\naltogether and parse the document with\\nlxml\\n: it’s a lot\\nfaster. But Soup Sieve lets you\\ncombine\\nCSS selectors with the\\nBeautiful Soup API.\\nAdvanced Soup Sieve features\\n¶\\nSoup Sieve offers a substantial API beyond the\\nselect()\\nand\\nselect_one()\\nmethods, and you can access most of that API through\\nthe\\n.css\\nattribute of\\nTag\\nor\\nBeautifulSoup\\n. What follows\\nis just a list of the supported methods; see\\nthe Soup Sieve\\ndocumentation\\nfor full\\ndocumentation.\\nThe\\niselect()\\nmethod works the same as\\nselect()\\n, but it\\nreturns a generator instead of a list:\\n[\\ntag\\n[\\n\\'id\\'\\n]\\nfor\\ntag\\nin\\nsoup\\n.\\ncss\\n.\\niselect\\n(\\n\".sister\"\\n)]\\n# [\\'link1\\', \\'link2\\', \\'link3\\']\\nThe\\nclosest()\\nmethod returns the nearest parent of a given\\nTag\\nthat matches a CSS selector, similar to Beautiful Soup’s\\nfind_parent()\\nmethod:\\nelsie\\n=\\nsoup\\n.\\ncss\\n.\\nselect_one\\n(\\n\".sister\"\\n)\\nelsie\\n.\\ncss\\n.\\nclosest\\n(\\n\"p.story\"\\n)\\n# <p class=\"story\">Once upon a time there were three little sisters; and their names were\\n# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\\n# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\\n# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\\n# and they lived at the bottom of a well.</p>\\nThe\\nmatch()\\nmethod returns a Boolean depending on whether or not a\\nspecific\\nTag\\nmatches a selector:\\n# elsie.css.match(\"#link1\")\\nTrue\\n# elsie.css.match(\"#link2\")\\nFalse\\nThe\\nfilter()\\nmethod returns the subset of a tag’s direct children\\nthat match a selector:\\n[\\ntag\\n.\\nstring\\nfor\\ntag\\nin\\nsoup\\n.\\nfind\\n(\\n\\'p\\'\\n,\\n\\'story\\'\\n)\\n.\\ncss\\n.\\nfilter\\n(\\n\\'a\\'\\n)]\\n# [\\'Elsie\\', \\'Lacie\\', \\'Tillie\\']\\nThe\\nescape()\\nmethod escapes CSS identifiers that would otherwise\\nbe invalid:\\nsoup\\n.\\ncss\\n.\\nescape\\n(\\n\"1-strange-identifier\"\\n)\\n# \\'\\\\\\\\31 -strange-identifier\\'\\nNamespaces in CSS selectors\\n¶\\nIf you’ve parsed XML that defines namespaces, you can use them in CSS\\nselectors.:\\nfrom\\nbs4\\nimport\\nBeautifulSoup\\nxml\\n=\\n\"\"\"<tag xmlns:ns1=\"http://namespace1/\" xmlns:ns2=\"http://namespace2/\">\\n<ns1:child>I\\'m in namespace 1</ns1:child>\\n<ns2:child>I\\'m in namespace 2</ns2:child>\\n</tag> \"\"\"\\nnamespace_soup\\n=\\nBeautifulSoup\\n(\\nxml\\n,\\n\"xml\"\\n)\\nnamespace_soup\\n.\\ncss\\n.\\nselect\\n(\\n\"child\"\\n)\\n# [<ns1:child>I\\'m in namespace 1</ns1:child>, <ns2:child>I\\'m in namespace 2</ns2:child>]\\nnamespace_soup\\n.\\ncss\\n.\\nselect\\n(\\n\"ns1|child\"\\n)\\n# [<ns1:child>I\\'m in namespace 1</ns1:child>]\\nBeautiful Soup tries to use namespace prefixes that make sense based\\non what it saw while parsing the document, but you can always provide\\nyour own dictionary of abbreviations:\\nnamespaces\\n=\\ndict\\n(\\nfirst\\n=\\n\"http://namespace1/\"\\n,\\nsecond\\n=\\n\"http://namespace2/\"\\n)\\nnamespace_soup\\n.\\ncss\\n.\\nselect\\n(\\n\"second|child\"\\n,\\nnamespaces\\n=\\nnamespaces\\n)\\n# [<ns1:child>I\\'m in namespace 2</ns1:child>]\\nHistory of CSS selector support\\n¶\\nThe\\n.css\\nproperty was added in Beautiful Soup 4.12.0. Prior to this,\\nonly the\\n.select()\\nand\\n.select_one()\\nconvenience methods were\\nsupported.\\nThe Soup Sieve integration was added in Beautiful Soup 4.7.0. Earlier\\nversions had the\\n.select()\\nmethod, but only the most commonly-used\\nCSS selectors were supported.\\nModifying the tree\\n¶\\nBeautiful Soup’s main strength is in searching the parse tree, but you\\ncan also modify the tree and write your changes as a new HTML or XML\\ndocument.\\nChanging tag names and attributes\\n¶\\nI covered this earlier, in\\nTag.attrs\\n, but it bears repeating. You\\ncan rename a tag, change the values of its attributes, add new\\nattributes, and delete attributes:\\nsoup\\n=\\nBeautifulSoup\\n(\\n\\'<b class=\"boldest\">Extremely bold</b>\\'\\n,\\n\\'html.parser\\'\\n)\\ntag\\n=\\nsoup\\n.\\nb\\ntag\\n.\\nname\\n=\\n\"blockquote\"\\ntag\\n[\\n\\'class\\'\\n]\\n=\\n\\'verybold\\'\\ntag\\n[\\n\\'id\\'\\n]\\n=\\n1\\ntag\\n# <blockquote class=\"verybold\" id=\"1\">Extremely bold</blockquote>\\ndel\\ntag\\n[\\n\\'class\\'\\n]\\ndel\\ntag\\n[\\n\\'id\\'\\n]\\ntag\\n# <blockquote>Extremely bold</blockquote>\\nModifying\\n.string\\n¶\\nIf you set a tag’s\\n.string\\nattribute to a new string, the tag’s contents are\\nreplaced with that string:\\nmarkup\\n=\\n\\'<a href=\"http://example.com/\">I linked to <i>example.com</i></a>\\'\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n)\\ntag\\n=\\nsoup\\n.\\na\\ntag\\n.\\nstring\\n=\\n\"New link text.\"\\ntag\\n# <a href=\"http://example.com/\">New link text.</a>\\nBe careful: if the tag contained other tags, they and all their\\ncontents will be destroyed.\\nappend()\\n¶\\nYou can add to a tag’s contents with\\nTag.append()\\n. It works just\\nlike calling\\n.append()\\non a Python list:\\nsoup\\n=\\nBeautifulSoup\\n(\\n\"<a>Foo</a>\"\\n,\\n\\'html.parser\\'\\n)\\nsoup\\n.\\na\\n.\\nappend\\n(\\n\"Bar\"\\n)\\nsoup\\n# <a>FooBar</a>\\nsoup\\n.\\na\\n.\\ncontents\\n# [\\'Foo\\', \\'Bar\\']\\nextend()\\n¶\\nStarting in Beautiful Soup 4.7.0,\\nTag\\nalso supports a method\\ncalled\\n.extend()\\n, which adds every element of a list to a\\nTag\\n,\\nin order:\\nsoup\\n=\\nBeautifulSoup\\n(\\n\"<a>Soup</a>\"\\n,\\n\\'html.parser\\'\\n)\\nsoup\\n.\\na\\n.\\nextend\\n([\\n\"\\'s\"\\n,\\n\" \"\\n,\\n\"on\"\\n])\\nsoup\\n# <a>Soup\\'s on</a>\\nsoup\\n.\\na\\n.\\ncontents\\n# [\\'Soup\\', \\'\\'s\\', \\' \\', \\'on\\']\\nNavigableString()\\nand\\n.new_tag()\\n¶\\nIf you need to add a string to a document, no problem–you can pass a\\nPython string in to\\nappend()\\n, or you can call the\\nNavigableString\\nconstructor:\\nfrom\\nbs4\\nimport\\nNavigableString\\nsoup\\n=\\nBeautifulSoup\\n(\\n\"<b></b>\"\\n,\\n\\'html.parser\\'\\n)\\ntag\\n=\\nsoup\\n.\\nb\\ntag\\n.\\nappend\\n(\\n\"Hello\"\\n)\\nnew_string\\n=\\nNavigableString\\n(\\n\" there\"\\n)\\ntag\\n.\\nappend\\n(\\nnew_string\\n)\\ntag\\n# <b>Hello there.</b>\\ntag\\n.\\ncontents\\n# [\\'Hello\\', \\' there\\']\\nIf you want to create a comment or some other subclass of\\nNavigableString\\n, just call the constructor:\\nfrom\\nbs4\\nimport\\nComment\\nnew_comment\\n=\\nComment\\n(\\n\"Nice to see you.\"\\n)\\ntag\\n.\\nappend\\n(\\nnew_comment\\n)\\ntag\\n# <b>Hello there<!--Nice to see you.--></b>\\ntag\\n.\\ncontents\\n# [\\'Hello\\', \\' there\\', \\'Nice to see you.\\']\\n(This is a new feature in Beautiful Soup 4.4.0.)\\nWhat if you need to create a whole new tag? The best solution is to\\ncall the factory method\\nBeautifulSoup.new_tag()\\n:\\nsoup\\n=\\nBeautifulSoup\\n(\\n\"<b></b>\"\\n,\\n\\'html.parser\\'\\n)\\noriginal_tag\\n=\\nsoup\\n.\\nb\\nnew_tag\\n=\\nsoup\\n.\\nnew_tag\\n(\\n\"a\"\\n,\\nhref\\n=\\n\"http://www.example.com\"\\n)\\noriginal_tag\\n.\\nappend\\n(\\nnew_tag\\n)\\noriginal_tag\\n# <b><a href=\"http://www.example.com\"></a></b>\\nnew_tag\\n.\\nstring\\n=\\n\"Link text.\"\\noriginal_tag\\n# <b><a href=\"http://www.example.com\">Link text.</a></b>\\nOnly the first argument, the tag name, is required.\\ninsert()\\n¶\\nTag.insert()\\nis just like\\nTag.append()\\n, except the new element\\ndoesn’t necessarily go at the end of its parent’s\\n.contents\\n. It’ll be inserted at whatever numeric position you\\nsay. It works just like\\n.insert()\\non a Python list:\\nmarkup\\n=\\n\\'<a href=\"http://example.com/\">I linked to <i>example.com</i></a>\\'\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n)\\ntag\\n=\\nsoup\\n.\\na\\ntag\\n.\\ninsert\\n(\\n1\\n,\\n\"but did not endorse \"\\n)\\ntag\\n# <a href=\"http://example.com/\">I linked to but did not endorse <i>example.com</i></a>\\ntag\\n.\\ncontents\\n# [\\'I linked to \\', \\'but did not endorse \\', <i>example.com</i>]\\ninsert_before()\\nand\\ninsert_after()\\n¶\\nThe\\ninsert_before()\\nmethod inserts tags or strings immediately\\nbefore something else in the parse tree:\\nsoup\\n=\\nBeautifulSoup\\n(\\n\"<b>leave</b>\"\\n,\\n\\'html.parser\\'\\n)\\ntag\\n=\\nsoup\\n.\\nnew_tag\\n(\\n\"i\"\\n)\\ntag\\n.\\nstring\\n=\\n\"Don\\'t\"\\nsoup\\n.\\nb\\n.\\nstring\\n.\\ninsert_before\\n(\\ntag\\n)\\nsoup\\n.\\nb\\n# <b><i>Don\\'t</i>leave</b>\\nThe\\ninsert_after()\\nmethod inserts tags or strings immediately\\nfollowing something else in the parse tree:\\ndiv\\n=\\nsoup\\n.\\nnew_tag\\n(\\n\\'div\\'\\n)\\ndiv\\n.\\nstring\\n=\\n\\'ever\\'\\nsoup\\n.\\nb\\n.\\ni\\n.\\ninsert_after\\n(\\n\" you \"\\n,\\ndiv\\n)\\nsoup\\n.\\nb\\n# <b><i>Don\\'t</i> you <div>ever</div> leave</b>\\nsoup\\n.\\nb\\n.\\ncontents\\n# [<i>Don\\'t</i>, \\' you\\', <div>ever</div>, \\'leave\\']\\nclear()\\n¶\\nTag.clear()\\nremoves the contents of a tag:\\nmarkup\\n=\\n\\'<a href=\"http://example.com/\">I linked to <i>example.com</i></a>\\'\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n)\\ntag\\n=\\nsoup\\n.\\na\\ntag\\n.\\nclear\\n()\\ntag\\n# <a href=\"http://example.com/\"></a>\\nextract()\\n¶\\nPageElement.extract()\\nremoves a tag or string from the tree. It\\nreturns the tag or string that was extracted:\\nmarkup\\n=\\n\\'<a href=\"http://example.com/\">I linked to <i>example.com</i></a>\\'\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n)\\na_tag\\n=\\nsoup\\n.\\na\\ni_tag\\n=\\nsoup\\n.\\ni\\n.\\nextract\\n()\\na_tag\\n# <a href=\"http://example.com/\">I linked to</a>\\ni_tag\\n# <i>example.com</i>\\nprint\\n(\\ni_tag\\n.\\nparent\\n)\\n# None\\nAt this point you effectively have two parse trees: one rooted at the\\nBeautifulSoup\\nobject you used to parse the document, and one rooted\\nat the tag that was extracted. You can go on to call\\nextract()\\non\\na child of the element you extracted:\\nmy_string\\n=\\ni_tag\\n.\\nstring\\n.\\nextract\\n()\\nmy_string\\n# \\'example.com\\'\\nprint\\n(\\nmy_string\\n.\\nparent\\n)\\n# None\\ni_tag\\n# <i></i>\\ndecompose()\\n¶\\nTag.decompose()\\nremoves a tag from the tree, then\\ncompletely\\ndestroys it and its contents\\n:\\nmarkup\\n=\\n\\'<a href=\"http://example.com/\">I linked to <i>example.com</i></a>\\'\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n)\\na_tag\\n=\\nsoup\\n.\\na\\ni_tag\\n=\\nsoup\\n.\\ni\\ni_tag\\n.\\ndecompose\\n()\\na_tag\\n# <a href=\"http://example.com/\">I linked to</a>\\nThe behavior of a decomposed\\nTag\\nor\\nNavigableString\\nis not\\ndefined and you should not use it for anything. If you’re not sure\\nwhether something has been decomposed, you can check its\\n.decomposed\\nproperty\\n(new in Beautiful Soup 4.9.0)\\n:\\ni_tag\\n.\\ndecomposed\\n# True\\na_tag\\n.\\ndecomposed\\n# False\\nreplace_with()\\n¶\\nPageElement.replace_with()\\nextracts a tag or string from the tree,\\nthen replaces it with one or more tags or strings of your choice:\\nmarkup\\n=\\n\\'<a href=\"http://example.com/\">I linked to <i>example.com</i></a>\\'\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n)\\na_tag\\n=\\nsoup\\n.\\na\\nnew_tag\\n=\\nsoup\\n.\\nnew_tag\\n(\\n\"b\"\\n)\\nnew_tag\\n.\\nstring\\n=\\n\"example.com\"\\na_tag\\n.\\ni\\n.\\nreplace_with\\n(\\nnew_tag\\n)\\na_tag\\n# <a href=\"http://example.com/\">I linked to <b>example.com</b></a>\\nbold_tag\\n=\\nsoup\\n.\\nnew_tag\\n(\\n\"b\"\\n)\\nbold_tag\\n.\\nstring\\n=\\n\"example\"\\ni_tag\\n=\\nsoup\\n.\\nnew_tag\\n(\\n\"i\"\\n)\\ni_tag\\n.\\nstring\\n=\\n\"net\"\\na_tag\\n.\\nb\\n.\\nreplace_with\\n(\\nbold_tag\\n,\\n\".\"\\n,\\ni_tag\\n)\\na_tag\\n# <a href=\"http://example.com/\">I linked to <b>example</b>.<i>net</i></a>\\nreplace_with()\\nreturns the tag or string that got replaced, so\\nthat you can examine it or add it back to another part of the tree.\\nThe ability to pass multiple arguments into replace_with() is new\\nin Beautiful Soup 4.10.0.\\nwrap()\\n¶\\nPageElement.wrap()\\nwraps an element in the\\nTag\\nobject you specify. It\\nreturns the new wrapper:\\nsoup\\n=\\nBeautifulSoup\\n(\\n\"<p>I wish I was bold.</p>\"\\n,\\n\\'html.parser\\'\\n)\\nsoup\\n.\\np\\n.\\nstring\\n.\\nwrap\\n(\\nsoup\\n.\\nnew_tag\\n(\\n\"b\"\\n))\\n# <b>I wish I was bold.</b>\\nsoup\\n.\\np\\n.\\nwrap\\n(\\nsoup\\n.\\nnew_tag\\n(\\n\"div\"\\n))\\n# <div><p><b>I wish I was bold.</b></p></div>\\nThis method is new in Beautiful Soup 4.0.5.\\nunwrap()\\n¶\\nTag.unwrap()\\nis the opposite of\\nwrap()\\n. It replaces a tag with\\nwhatever’s inside that tag. It’s good for stripping out markup:\\nmarkup\\n=\\n\\'<a href=\"http://example.com/\">I linked to <i>example.com</i></a>\\'\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n)\\na_tag\\n=\\nsoup\\n.\\na\\na_tag\\n.\\ni\\n.\\nunwrap\\n()\\na_tag\\n# <a href=\"http://example.com/\">I linked to example.com</a>\\nLike\\nreplace_with()\\n,\\nunwrap()\\nreturns the tag\\nthat was replaced.\\nsmooth()\\n¶\\nAfter calling a bunch of methods that modify the parse tree, you may end up\\nwith two or more\\nNavigableString\\nobjects next to each other.\\nBeautiful Soup doesn’t have any problems with this, but since it can’t happen\\nin a freshly parsed document, you might not expect behavior like the\\nfollowing:\\nsoup\\n=\\nBeautifulSoup\\n(\\n\"<p>A one</p>\"\\n,\\n\\'html.parser\\'\\n)\\nsoup\\n.\\np\\n.\\nappend\\n(\\n\", a two\"\\n)\\nsoup\\n.\\np\\n.\\ncontents\\n# [\\'A one\\', \\', a two\\']\\nprint\\n(\\nsoup\\n.\\np\\n.\\nencode\\n())\\n# b\\'<p>A one, a two</p>\\'\\nprint\\n(\\nsoup\\n.\\np\\n.\\nprettify\\n())\\n# <p>\\n# A one\\n# , a two\\n# </p>\\nYou can call\\nTag.smooth()\\nto clean up the parse tree by consolidating adjacent strings:\\nsoup\\n.\\nsmooth\\n()\\nsoup\\n.\\np\\n.\\ncontents\\n# [\\'A one, a two\\']\\nprint\\n(\\nsoup\\n.\\np\\n.\\nprettify\\n())\\n# <p>\\n# A one, a two\\n# </p>\\nThis method is new in Beautiful Soup 4.8.0.\\nOutput\\n¶\\nPretty-printing\\n¶\\nThe\\nprettify()\\nmethod will turn a Beautiful Soup parse tree into a\\nnicely formatted Unicode string, with a separate line for each\\ntag and each string:\\nmarkup\\n=\\n\\'<html><head><body><a href=\"http://example.com/\">I linked to <i>example.com</i></a>\\'\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n)\\nsoup\\n.\\nprettify\\n()\\n# \\'<html>\\\\n <head>\\\\n </head>\\\\n <body>\\\\n <a href=\"http://example.com/\">\\\\n...\\'\\nprint\\n(\\nsoup\\n.\\nprettify\\n())\\n# <html>\\n# <head>\\n# </head>\\n# <body>\\n# <a href=\"http://example.com/\">\\n# I linked to\\n# <i>\\n# example.com\\n# </i>\\n# </a>\\n# </body>\\n# </html>\\nYou can call\\nprettify()\\non the top-level\\nBeautifulSoup\\nobject,\\nor on any of its\\nTag\\nobjects:\\nprint\\n(\\nsoup\\n.\\na\\n.\\nprettify\\n())\\n# <a href=\"http://example.com/\">\\n# I linked to\\n# <i>\\n# example.com\\n# </i>\\n# </a>\\nSince it adds whitespace (in the form of newlines),\\nprettify()\\nchanges the meaning of an HTML document and should not be used to\\nreformat one. The goal of\\nprettify()\\nis to help you visually\\nunderstand the structure of the documents you work with.\\nNon-pretty printing\\n¶\\nIf you just want a string, with no fancy formatting, you can call\\nstr()\\non a\\nBeautifulSoup\\nobject, or on a\\nTag\\nwithin it:\\nstr\\n(\\nsoup\\n)\\n# \\'<html><head></head><body><a href=\"http://example.com/\">I linked to <i>example.com</i></a></body></html>\\'\\nstr\\n(\\nsoup\\n.\\na\\n)\\n# \\'<a href=\"http://example.com/\">I linked to <i>example.com</i></a>\\'\\nThe\\nstr()\\nfunction returns a string encoded in UTF-8. See\\nEncodings\\nfor other options.\\nYou can also call\\nencode()\\nto get a bytestring, and\\ndecode()\\nto get Unicode.\\nOutput formatters\\n¶\\nIf you give Beautiful Soup a document that contains HTML entities like\\n“&lquot;”, they’ll be converted to Unicode characters:\\nsoup\\n=\\nBeautifulSoup\\n(\\n\"&ldquo;Dammit!&rdquo; he said.\"\\n,\\n\\'html.parser\\'\\n)\\nstr\\n(\\nsoup\\n)\\n# \\'“Dammit!” he said.\\'\\nIf you then convert the document to a bytestring, the Unicode characters\\nwill be encoded as UTF-8. You won’t get the HTML entities back:\\nsoup\\n.\\nencode\\n(\\n\"utf8\"\\n)\\n# b\\'\\\\xe2\\\\x80\\\\x9cDammit!\\\\xe2\\\\x80\\\\x9d he said.\\'\\nBy default, the only characters that are escaped upon output are bare\\nampersands and angle brackets. These get turned into “&amp;”, “&lt;”,\\nand “&gt;”, so that Beautiful Soup doesn’t inadvertently generate\\ninvalid HTML or XML:\\nsoup\\n=\\nBeautifulSoup\\n(\\n\"<p>The law firm of Dewey, Cheatem, & Howe</p>\"\\n,\\n\\'html.parser\\'\\n)\\nsoup\\n.\\np\\n# <p>The law firm of Dewey, Cheatem, &amp; Howe</p>\\nsoup\\n=\\nBeautifulSoup\\n(\\n\\'<a href=\"http://example.com/?foo=val1&bar=val2\">A link</a>\\'\\n,\\n\\'html.parser\\'\\n)\\nsoup\\n.\\na\\n# <a href=\"http://example.com/?foo=val1&amp;bar=val2\">A link</a>\\nYou can change this behavior by providing a value for the\\nformatter\\nargument to\\nprettify()\\n,\\nencode()\\n, or\\ndecode()\\n. Beautiful Soup recognizes five possible values for\\nformatter\\n.\\nThe default is\\nformatter=\"minimal\"\\n. Strings will only be processed\\nenough to ensure that Beautiful Soup generates valid HTML/XML:\\nfrench\\n=\\n\"<p>Il a dit &lt;&lt;Sacr&eacute; bleu!&gt;&gt;</p>\"\\nsoup\\n=\\nBeautifulSoup\\n(\\nfrench\\n,\\n\\'html.parser\\'\\n)\\nprint\\n(\\nsoup\\n.\\nprettify\\n(\\nformatter\\n=\\n\"minimal\"\\n))\\n# <p>\\n# Il a dit &lt;&lt;Sacré bleu!&gt;&gt;\\n# </p>\\nIf you pass in\\nformatter=\"html\"\\n, Beautiful Soup will convert\\nUnicode characters to HTML entities whenever possible:\\nprint\\n(\\nsoup\\n.\\nprettify\\n(\\nformatter\\n=\\n\"html\"\\n))\\n# <p>\\n# Il a dit &lt;&lt;Sacr&eacute; bleu!&gt;&gt;\\n# </p>\\nIf you pass in\\nformatter=\"html5\"\\n, it’s similar to\\nformatter=\"html\"\\n, but Beautiful Soup will\\nomit the closing slash in HTML void tags like “br”:\\nbr\\n=\\nBeautifulSoup\\n(\\n\"<br>\"\\n,\\n\\'html.parser\\'\\n)\\n.\\nbr\\nprint\\n(\\nbr\\n.\\nencode\\n(\\nformatter\\n=\\n\"html\"\\n))\\n# b\\'<br/>\\'\\nprint\\n(\\nbr\\n.\\nencode\\n(\\nformatter\\n=\\n\"html5\"\\n))\\n# b\\'<br>\\'\\nIn addition, any attributes whose values are the empty string\\nwill become HTML-style Boolean attributes:\\noption\\n=\\nBeautifulSoup\\n(\\n\\'<option selected=\"\"></option>\\'\\n)\\n.\\noption\\nprint\\n(\\noption\\n.\\nencode\\n(\\nformatter\\n=\\n\"html\"\\n))\\n# b\\'<option selected=\"\"></option>\\'\\nprint\\n(\\noption\\n.\\nencode\\n(\\nformatter\\n=\\n\"html5\"\\n))\\n# b\\'<option selected></option>\\'\\n(This behavior is new as of Beautiful Soup 4.10.0.)\\nIf you pass in\\nformatter=None\\n, Beautiful Soup will not modify\\nstrings at all on output. This is the fastest option, but it may lead\\nto Beautiful Soup generating invalid HTML/XML, as in these examples:\\nprint\\n(\\nsoup\\n.\\nprettify\\n(\\nformatter\\n=\\nNone\\n))\\n# <p>\\n# Il a dit <<Sacré bleu!>>\\n# </p>\\nlink_soup\\n=\\nBeautifulSoup\\n(\\n\\'<a href=\"http://example.com/?foo=val1&bar=val2\">A link</a>\\'\\n,\\n\\'html.parser\\'\\n)\\nprint\\n(\\nlink_soup\\n.\\na\\n.\\nencode\\n(\\nformatter\\n=\\nNone\\n))\\n# b\\'<a href=\"http://example.com/?foo=val1&bar=val2\">A link</a>\\'\\nFormatter objects\\n¶\\nIf you need more sophisticated control over your output, you can\\ninstantiate one of Beautiful Soup’s formatter classes and pass that\\nobject in as\\nformatter\\n.\\nclass\\nbs4.\\nHTMLFormatter\\n¶\\nUsed to customize the formatting rules for HTML documents.\\nHere’s a formatter that converts strings to uppercase, whether they\\noccur in a string object or an attribute value:\\nfrom\\nbs4.formatter\\nimport\\nHTMLFormatter\\ndef\\nuppercase\\n(\\nstr\\n):\\nreturn\\nstr\\n.\\nupper\\n()\\nformatter\\n=\\nHTMLFormatter\\n(\\nuppercase\\n)\\nprint\\n(\\nsoup\\n.\\nprettify\\n(\\nformatter\\n=\\nformatter\\n))\\n# <p>\\n# IL A DIT <<SACRÉ BLEU!>>\\n# </p>\\nprint\\n(\\nlink_soup\\n.\\na\\n.\\nprettify\\n(\\nformatter\\n=\\nformatter\\n))\\n# <a href=\"HTTP://EXAMPLE.COM/?FOO=VAL1&BAR=VAL2\">\\n# A LINK\\n# </a>\\nHere’s a formatter that increases the indentation width when pretty-printing:\\nformatter\\n=\\nHTMLFormatter\\n(\\nindent\\n=\\n8\\n)\\nprint\\n(\\nlink_soup\\n.\\na\\n.\\nprettify\\n(\\nformatter\\n=\\nformatter\\n))\\n# <a href=\"http://example.com/?foo=val1&bar=val2\">\\n# A link\\n# </a>\\nclass\\nbs4.\\nXMLFormatter\\n¶\\nUsed to customize the formatting rules for XML documents.\\nWriting your own formatter\\n¶\\nSubclassing\\nHTMLFormatter\\nor\\nXMLFormatter\\nwill\\ngive you even more control over the output. For example, Beautiful\\nSoup sorts the attributes in every tag by default:\\nattr_soup\\n=\\nBeautifulSoup\\n(\\nb\\n\\'<p z=\"1\" m=\"2\" a=\"3\"></p>\\'\\n,\\n\\'html.parser\\'\\n)\\nprint\\n(\\nattr_soup\\n.\\np\\n.\\nencode\\n())\\n# <p a=\"3\" m=\"2\" z=\"1\"></p>\\nTo turn this off, you can subclass the\\nFormatter.attributes()\\nmethod, which controls which attributes are output and in what\\norder. This implementation also filters out the attribute called “m”\\nwhenever it appears:\\nclass\\nUnsortedAttributes\\n(\\nHTMLFormatter\\n):\\ndef\\nattributes\\n(\\nself\\n,\\ntag\\n):\\nfor\\nk\\n,\\nv\\nin\\ntag\\n.\\nattrs\\n.\\nitems\\n():\\nif\\nk\\n==\\n\\'m\\'\\n:\\ncontinue\\nyield\\nk\\n,\\nv\\nprint\\n(\\nattr_soup\\n.\\np\\n.\\nencode\\n(\\nformatter\\n=\\nUnsortedAttributes\\n()))\\n# <p z=\"1\" a=\"3\"></p>\\nOne last caveat: if you create a\\nCData\\nobject, the text inside\\nthat object is always presented\\nexactly as it appears, with no\\nformatting\\n. Beautiful Soup will call your entity substitution\\nfunction, just in case you’ve written a custom function that counts\\nall the strings in the document or something, but it will ignore the\\nreturn value:\\nfrom\\nbs4.element\\nimport\\nCData\\nsoup\\n=\\nBeautifulSoup\\n(\\n\"<a></a>\"\\n,\\n\\'html.parser\\'\\n)\\nsoup\\n.\\na\\n.\\nstring\\n=\\nCData\\n(\\n\"one < three\"\\n)\\nprint\\n(\\nsoup\\n.\\na\\n.\\nprettify\\n(\\nformatter\\n=\\n\"html\"\\n))\\n# <a>\\n# <![CDATA[one < three]]>\\n# </a>\\nget_text()\\n¶\\nIf you only want the human-readable text inside a document or tag, you can use the\\nget_text()\\nmethod. It returns all the text in a document or\\nbeneath a tag, as a single Unicode string:\\nmarkup\\n=\\n\\'<a href=\"http://example.com/\">\\n\\\\n\\nI linked to <i>example.com</i>\\n\\\\n\\n</a>\\'\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n)\\nsoup\\n.\\nget_text\\n()\\n\\'\\n\\\\n\\nI linked to example.com\\n\\\\n\\n\\'\\nsoup\\n.\\ni\\n.\\nget_text\\n()\\n\\'example.com\\'\\nYou can specify a string to be used to join the bits of text\\ntogether:\\n# soup.get_text(\"|\")\\n\\'\\n\\\\n\\nI linked to |example.com|\\n\\\\n\\n\\'\\nYou can tell Beautiful Soup to strip whitespace from the beginning and\\nend of each bit of text:\\n# soup.get_text(\"|\", strip=True)\\n\\'I linked to|example.com\\'\\nBut at that point you might want to use the\\n.stripped_strings\\ngenerator instead, and process the text yourself:\\n[\\ntext\\nfor\\ntext\\nin\\nsoup\\n.\\nstripped_strings\\n]\\n# [\\'I linked to\\', \\'example.com\\']\\nAs of Beautiful Soup version 4.9.0, when lxml or html.parser are in\\nuse, the contents of <script>, <style>, and <template>\\ntags are generally not considered to be ‘text’, since those tags are not part of\\nthe human-visible content of the page.\\nAs of Beautiful Soup version 4.10.0, you can call get_text(),\\n.strings, or .stripped_strings on a NavigableString object. It will\\neither return the object itself, or nothing, so the only reason to do\\nthis is when you’re iterating over a mixed list.\\nSpecifying the parser to use\\n¶\\nIf you just need to parse some HTML, you can dump the markup into the\\nBeautifulSoup\\nconstructor, and it’ll probably be fine. Beautiful\\nSoup will pick a parser for you and parse the data. But there are a\\nfew additional arguments you can pass in to the constructor to change\\nwhich parser is used.\\nThe first argument to the\\nBeautifulSoup\\nconstructor is a string or\\nan open filehandle—the source of the markup you want parsed. The second\\nargument is\\nhow\\nyou’d like the markup parsed.\\nIf you don’t specify anything, you’ll get the best HTML parser that’s\\ninstalled. Beautiful Soup ranks lxml’s parser as being the best, then\\nhtml5lib’s, then Python’s built-in parser. You can override this by\\nspecifying one of the following:\\nWhat type of markup you want to parse. Currently supported values are\\n“html”, “xml”, and “html5”.\\nThe name of the parser library you want to use. Currently supported\\noptions are “lxml”, “html5lib”, and “html.parser” (Python’s\\nbuilt-in HTML parser).\\nThe section\\nInstalling a parser\\ncontrasts the supported parsers.\\nIf you don’t have an appropriate parser installed, Beautiful Soup will\\nignore your request and pick a different parser. Right now, the only\\nsupported XML parser is lxml. If you don’t have lxml installed, asking\\nfor an XML parser won’t give you one, and asking for “lxml” won’t work\\neither.\\nDifferences between parsers\\n¶\\nBeautiful Soup presents the same interface to a number of different\\nparsers, but each parser is different. Different parsers will create\\ndifferent parse trees from the same document. The biggest differences\\nare between the HTML parsers and the XML parsers. Here’s a short\\ndocument, parsed as HTML using the parser that comes with Python:\\nBeautifulSoup\\n(\\n\"<a><b/></a>\"\\n,\\n\"html.parser\"\\n)\\n# <a><b></b></a>\\nSince a standalone <b/> tag is not valid HTML, html.parser turns it into\\na <b></b> tag pair.\\nHere’s the same document parsed as XML (running this requires that you\\nhave lxml installed). Note that the standalone <b/> tag is left alone, and\\nthat the document is given an XML declaration instead of being put\\ninto an <html> tag.:\\nprint\\n(\\nBeautifulSoup\\n(\\n\"<a><b/></a>\"\\n,\\n\"xml\"\\n))\\n# <?xml version=\"1.0\" encoding=\"utf-8\"?>\\n# <a><b/></a>\\nThere are also differences between HTML parsers. If you give Beautiful\\nSoup a perfectly-formed HTML document, these differences won’t\\nmatter. One parser will be faster than another, but they’ll all give\\nyou a data structure that looks exactly like the original HTML\\ndocument.\\nBut if the document is not perfectly-formed, different parsers will\\ngive different results. Here’s a short, invalid document parsed using\\nlxml’s HTML parser. Note that the <a> tag gets wrapped in <body> and\\n<html> tags, and the dangling </p> tag is simply ignored:\\nBeautifulSoup\\n(\\n\"<a></p>\"\\n,\\n\"lxml\"\\n)\\n# <html><body><a></a></body></html>\\nHere’s the same document parsed using html5lib:\\nBeautifulSoup\\n(\\n\"<a></p>\"\\n,\\n\"html5lib\"\\n)\\n# <html><head></head><body><a><p></p></a></body></html>\\nInstead of ignoring the dangling </p> tag, html5lib pairs it with an\\nopening <p> tag. html5lib also adds an empty <head> tag; lxml didn’t\\nbother.\\nHere’s the same document parsed with Python’s built-in HTML\\nparser:\\nBeautifulSoup\\n(\\n\"<a></p>\"\\n,\\n\"html.parser\"\\n)\\n# <a></a>\\nLike lxml, this parser ignores the closing </p> tag. Unlike\\nhtml5lib or lxml, this parser makes no attempt to create a\\nwell-formed HTML document by adding <html> or <body> tags.\\nSince the document “<a></p>” is invalid, none of these techniques is\\nthe ‘correct’ way to handle it. The html5lib parser uses techniques\\nthat are part of the HTML5 standard, so it has the best claim on being\\nthe ‘correct’ way, but all three techniques are legitimate.\\nDifferences between parsers can affect your script. If you’re planning\\non distributing your script to other people, or running it on multiple\\nmachines, you should specify a parser in the\\nBeautifulSoup\\nconstructor. That will reduce the chances that your users parse a\\ndocument differently from the way you parse it.\\nEncodings\\n¶\\nAny HTML or XML document is written in a specific encoding like ASCII\\nor UTF-8. But when you load that document into Beautiful Soup, you’ll\\ndiscover it’s been converted to Unicode:\\nmarkup\\n=\\n\"<h1>Sacr\\n\\\\xc3\\\\xa9\\nbleu!</h1>\"\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n)\\nsoup\\n.\\nh1\\n# <h1>Sacré bleu!</h1>\\nsoup\\n.\\nh1\\n.\\nstring\\n# \\'Sacr\\\\xe9 bleu!\\'\\nIt’s not magic. (That sure would be nice.) Beautiful Soup uses a\\nsub-library called\\nUnicode, Dammit\\nto detect a document’s encoding\\nand convert it to Unicode. The autodetected encoding is available as\\nthe\\n.original_encoding\\nattribute of the\\nBeautifulSoup\\nobject:\\nsoup\\n.\\noriginal_encoding\\n\\'utf-8\\'\\nUnicode, Dammit guesses correctly most of the time, but sometimes it\\nmakes mistakes. Sometimes it guesses correctly, but only after a\\nbyte-by-byte search of the document that takes a very long time. If\\nyou happen to know a document’s encoding ahead of time, you can avoid\\nmistakes and delays by passing it to the\\nBeautifulSoup\\nconstructor\\nas\\nfrom_encoding\\n.\\nHere’s a document written in ISO-8859-8. The document is so short that\\nUnicode, Dammit can’t get a lock on it, and misidentifies it as\\nISO-8859-7:\\nmarkup\\n=\\nb\\n\"<h1>\\n\\\\xed\\\\xe5\\\\xec\\\\xf9\\n</h1>\"\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n)\\nprint\\n(\\nsoup\\n.\\nh1\\n)\\n# <h1>νεμω</h1>\\nprint\\n(\\nsoup\\n.\\noriginal_encoding\\n)\\n# iso-8859-7\\nWe can fix this by passing in the correct\\nfrom_encoding\\n:\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n,\\nfrom_encoding\\n=\\n\"iso-8859-8\"\\n)\\nprint\\n(\\nsoup\\n.\\nh1\\n)\\n# <h1>םולש</h1>\\nprint\\n(\\nsoup\\n.\\noriginal_encoding\\n)\\n# iso8859-8\\nIf you don’t know what the correct encoding is, but you know that\\nUnicode, Dammit is guessing wrong, you can pass the wrong guesses in\\nas\\nexclude_encodings\\n:\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n,\\nexclude_encodings\\n=\\n[\\n\"iso-8859-7\"\\n])\\nprint\\n(\\nsoup\\n.\\nh1\\n)\\n# <h1>םולש</h1>\\nprint\\n(\\nsoup\\n.\\noriginal_encoding\\n)\\n# WINDOWS-1255\\nWindows-1255 isn’t 100% correct, but that encoding is a compatible\\nsuperset of ISO-8859-8, so it’s close enough. (\\nexclude_encodings\\nis a new feature in Beautiful Soup 4.4.0.)\\nIn rare cases (usually when a UTF-8 document contains text written in\\na completely different encoding), the only way to get Unicode may be\\nto replace some characters with the special Unicode character\\n“REPLACEMENT CHARACTER” (U+FFFD, �). If Unicode, Dammit needs to do\\nthis, it will set the\\n.contains_replacement_characters\\nattribute\\nto\\nTrue\\non the\\nUnicodeDammit\\nor\\nBeautifulSoup\\nobject. This\\nlets you know that the Unicode representation is not an exact\\nrepresentation of the original–some data was lost. If a document\\ncontains �, but\\n.contains_replacement_characters\\nis\\nFalse\\n,\\nyou’ll know that the � was there originally (as it is in this\\nparagraph) and doesn’t stand in for missing data.\\nOutput encoding\\n¶\\nWhen you write out an output document from Beautiful Soup, you get a UTF-8\\ndocument, even if the input document wasn’t in UTF-8 to begin with. Here’s a\\ndocument written in the Latin-1 encoding:\\nmarkup\\n=\\nb\\n\\'\\'\\'\\n<html>\\n<head>\\n<meta content=\"text/html; charset=ISO-Latin-1\" http-equiv=\"Content-type\" />\\n</head>\\n<body>\\n<p>Sacr\\n\\\\xe9\\nbleu!</p>\\n</body>\\n</html>\\n\\'\\'\\'\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n)\\nprint\\n(\\nsoup\\n.\\nprettify\\n())\\n# <html>\\n# <head>\\n# <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-type\" />\\n# </head>\\n# <body>\\n# <p>\\n# Sacré bleu!\\n# </p>\\n# </body>\\n# </html>\\nNote that the <meta> tag has been rewritten to reflect the fact that\\nthe document is now in UTF-8.\\nIf you don’t want UTF-8, you can pass an encoding into\\nprettify()\\n:\\nprint\\n(\\nsoup\\n.\\nprettify\\n(\\n\"latin-1\"\\n))\\n# <html>\\n# <head>\\n# <meta content=\"text/html; charset=latin-1\" http-equiv=\"Content-type\" />\\n# ...\\nYou can also call encode() on the\\nBeautifulSoup\\nobject, or any\\nelement in the soup, just as if it were a Python string:\\nsoup\\n.\\np\\n.\\nencode\\n(\\n\"latin-1\"\\n)\\n# b\\'<p>Sacr\\\\xe9 bleu!</p>\\'\\nsoup\\n.\\np\\n.\\nencode\\n(\\n\"utf-8\"\\n)\\n# b\\'<p>Sacr\\\\xc3\\\\xa9 bleu!</p>\\'\\nAny characters that can’t be represented in your chosen encoding will\\nbe converted into numeric XML entity references. Here’s a document\\nthat includes the Unicode character SNOWMAN:\\nmarkup\\n=\\nu\\n\"<b>\\n\\\\N{SNOWMAN}\\n</b>\"\\nsnowman_soup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n)\\ntag\\n=\\nsnowman_soup\\n.\\nb\\nThe SNOWMAN character can be part of a UTF-8 document (it looks like\\n☃), but there’s no representation for that character in ISO-Latin-1 or\\nASCII, so it’s converted into “&#9731” for those encodings:\\nprint\\n(\\ntag\\n.\\nencode\\n(\\n\"utf-8\"\\n))\\n# b\\'<b>\\\\xe2\\\\x98\\\\x83</b>\\'\\nprint\\n(\\ntag\\n.\\nencode\\n(\\n\"latin-1\"\\n))\\n# b\\'<b>&#9731;</b>\\'\\nprint\\n(\\ntag\\n.\\nencode\\n(\\n\"ascii\"\\n))\\n# b\\'<b>&#9731;</b>\\'\\nUnicode, Dammit\\n¶\\nYou can use Unicode, Dammit without using Beautiful Soup. It’s useful\\nwhenever you have data in an unknown encoding and you just want it to\\nbecome Unicode:\\nfrom\\nbs4\\nimport\\nUnicodeDammit\\ndammit\\n=\\nUnicodeDammit\\n(\\nb\\n\"\\n\\\\xc2\\\\xab\\nSacr\\n\\\\xc3\\\\xa9\\nbleu!\\n\\\\xc2\\\\xbb\\n\"\\n)\\nprint\\n(\\ndammit\\n.\\nunicode_markup\\n)\\n# «Sacré bleu!»\\ndammit\\n.\\noriginal_encoding\\n# \\'utf-8\\'\\nUnicode, Dammit’s guesses will get a lot more accurate if you install\\none of these Python libraries:\\ncharset-normalizer\\n,\\nchardet\\n, or\\ncchardet\\n. The more data you give Unicode, Dammit, the more\\naccurately it will guess. If you have your own suspicions as to what\\nthe encoding might be, you can pass them in as a list:\\ndammit\\n=\\nUnicodeDammit\\n(\\n\"Sacr\\n\\\\xe9\\nbleu!\"\\n,\\n[\\n\"latin-1\"\\n,\\n\"iso-8859-1\"\\n])\\nprint\\n(\\ndammit\\n.\\nunicode_markup\\n)\\n# Sacré bleu!\\ndammit\\n.\\noriginal_encoding\\n# \\'latin-1\\'\\nUnicode, Dammit has two special features that Beautiful Soup doesn’t\\nuse.\\nSmart quotes\\n¶\\nYou can use Unicode, Dammit to convert Microsoft smart quotes to HTML or XML\\nentities:\\nmarkup\\n=\\nb\\n\"<p>I just\\n\\\\x93\\nlove\\n\\\\x94\\nMicrosoft Word\\n\\\\x92\\ns smart quotes</p>\"\\nUnicodeDammit\\n(\\nmarkup\\n,\\n[\\n\"windows-1252\"\\n],\\nsmart_quotes_to\\n=\\n\"html\"\\n)\\n.\\nunicode_markup\\n# \\'<p>I just &ldquo;love&rdquo; Microsoft Word&rsquo;s smart quotes</p>\\'\\nUnicodeDammit\\n(\\nmarkup\\n,\\n[\\n\"windows-1252\"\\n],\\nsmart_quotes_to\\n=\\n\"xml\"\\n)\\n.\\nunicode_markup\\n# \\'<p>I just &#x201C;love&#x201D; Microsoft Word&#x2019;s smart quotes</p>\\'\\nYou can also convert Microsoft smart quotes to ASCII quotes:\\nUnicodeDammit\\n(\\nmarkup\\n,\\n[\\n\"windows-1252\"\\n],\\nsmart_quotes_to\\n=\\n\"ascii\"\\n)\\n.\\nunicode_markup\\n# \\'<p>I just \"love\" Microsoft Word\\\\\\'s smart quotes</p>\\'\\nHopefully you’ll find this feature useful, but Beautiful Soup doesn’t\\nuse it. Beautiful Soup prefers the default behavior, which is to\\nconvert Microsoft smart quotes to Unicode characters along with\\neverything else:\\nUnicodeDammit\\n(\\nmarkup\\n,\\n[\\n\"windows-1252\"\\n])\\n.\\nunicode_markup\\n# \\'<p>I just “love” Microsoft Word’s smart quotes</p>\\'\\nInconsistent encodings\\n¶\\nSometimes a document is mostly in UTF-8, but contains Windows-1252\\ncharacters such as (again) Microsoft smart quotes. This can happen\\nwhen a website includes data from multiple sources. You can use\\nUnicodeDammit.detwingle()\\nto turn such a document into pure\\nUTF-8. Here’s a simple example:\\nsnowmen\\n=\\n(\\nu\\n\"\\n\\\\N{SNOWMAN}\\n\"\\n*\\n3\\n)\\nquote\\n=\\n(\\nu\\n\"\\n\\\\N{LEFT DOUBLE QUOTATION MARK}\\nI like snowmen!\\n\\\\N{RIGHT DOUBLE QUOTATION MARK}\\n\"\\n)\\ndoc\\n=\\nsnowmen\\n.\\nencode\\n(\\n\"utf8\"\\n)\\n+\\nquote\\n.\\nencode\\n(\\n\"windows_1252\"\\n)\\nThis document is a mess. The snowmen are in UTF-8 and the quotes are\\nin Windows-1252. You can display the snowmen or the quotes, but not\\nboth:\\nprint\\n(\\ndoc\\n)\\n# ☃☃☃�I like snowmen!�\\nprint\\n(\\ndoc\\n.\\ndecode\\n(\\n\"windows-1252\"\\n))\\n# â˜ƒâ˜ƒâ˜ƒ“I like snowmen!”\\nDecoding the document as UTF-8 raises a\\nUnicodeDecodeError\\n, and\\ndecoding it as Windows-1252 gives you gibberish. Fortunately,\\nUnicodeDammit.detwingle()\\nwill convert the string to pure UTF-8,\\nallowing you to decode it to Unicode and display the snowmen and quote\\nmarks simultaneously:\\nnew_doc\\n=\\nUnicodeDammit\\n.\\ndetwingle\\n(\\ndoc\\n)\\nprint\\n(\\nnew_doc\\n.\\ndecode\\n(\\n\"utf8\"\\n))\\n# ☃☃☃“I like snowmen!”\\nUnicodeDammit.detwingle()\\nonly knows how to handle Windows-1252\\nembedded in UTF-8 (or vice versa, I suppose), but this is the most\\ncommon case.\\nNote that you must know to call\\nUnicodeDammit.detwingle()\\non your\\ndata before passing it into\\nBeautifulSoup\\nor the\\nUnicodeDammit\\nconstructor. Beautiful Soup assumes that a document has a single\\nencoding, whatever it might be. If you pass it a document that\\ncontains both UTF-8 and Windows-1252, it’s likely to think the whole\\ndocument is Windows-1252, and the document will come out looking like\\nâ˜ƒâ˜ƒâ˜ƒ“I\\nlike\\nsnowmen!”\\n.\\nUnicodeDammit.detwingle()\\nis new in Beautiful Soup 4.1.0.\\nLine numbers\\n¶\\nThe\\nhtml.parser\\nand\\nhtml5lib\\nparsers can keep track of where in\\nthe original document each\\nTag\\nwas found. You can access this\\ninformation as\\nTag.sourceline\\n(line number) and\\nTag.sourcepos\\n(position of the start tag within a line):\\nmarkup\\n=\\n\"<p\\n\\\\n\\n>Paragraph 1</p>\\n\\\\n\\n<p>Paragraph 2</p>\"\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n)\\nfor\\ntag\\nin\\nsoup\\n.\\nfind_all\\n(\\n\\'p\\'\\n):\\nprint\\n(\\nrepr\\n((\\ntag\\n.\\nsourceline\\n,\\ntag\\n.\\nsourcepos\\n,\\ntag\\n.\\nstring\\n)))\\n# (1, 0, \\'Paragraph 1\\')\\n# (3, 4, \\'Paragraph 2\\')\\nNote that the two parsers mean slightly different things by\\nsourceline\\nand\\nsourcepos\\n. For html.parser, these numbers\\nrepresent the position of the initial less-than sign. For html5lib,\\nthese numbers represent the position of the final greater-than sign:\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html5lib\\'\\n)\\nfor\\ntag\\nin\\nsoup\\n.\\nfind_all\\n(\\n\\'p\\'\\n):\\nprint\\n(\\nrepr\\n((\\ntag\\n.\\nsourceline\\n,\\ntag\\n.\\nsourcepos\\n,\\ntag\\n.\\nstring\\n)))\\n# (2, 0, \\'Paragraph 1\\')\\n# (3, 6, \\'Paragraph 2\\')\\nYou can shut off this feature by passing\\nstore_line_numbers=False\\ninto the\\nBeautifulSoup\\nconstructor:\\nmarkup\\n=\\n\"<p\\n\\\\n\\n>Paragraph 1</p>\\n\\\\n\\n<p>Paragraph 2</p>\"\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n,\\nstore_line_numbers\\n=\\nFalse\\n)\\nprint\\n(\\nsoup\\n.\\np\\n.\\nsourceline\\n)\\n# None\\nThis feature is new in 4.8.1, and the parsers based on lxml don’t\\nsupport it.\\nComparing objects for equality\\n¶\\nBeautiful Soup says that two\\nNavigableString\\nor\\nTag\\nobjects\\nare equal when they represent the same HTML or XML markup, even if their\\nattributes are in a different order or they live in different parts of the\\nobject tree. In this example, the two <b> tags are treated as equal, because\\nthey both look like “<b>pizza</b>”:\\nmarkup\\n=\\n\"<p>I want <b>pizza</b> and more <b>pizza</b>!</p>\"\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n)\\nfirst_b\\n,\\nsecond_b\\n=\\nsoup\\n.\\nfind_all\\n(\\n\\'b\\'\\n)\\nprint\\n(\\nfirst_b\\n==\\nsecond_b\\n)\\n# True\\nprint\\n(\\nfirst_b\\n.\\nprevious_element\\n==\\nsecond_b\\n.\\nprevious_element\\n)\\n# False\\nIf you want to see whether two variables refer to exactly the same\\nobject, use\\nis\\n:\\nprint\\n(\\nfirst_b\\nis\\nsecond_b\\n)\\n# False\\nCopying Beautiful Soup objects\\n¶\\nYou can use\\ncopy.copy()\\nto create a copy of any\\nTag\\nor\\nNavigableString\\n:\\nimport\\ncopy\\np_copy\\n=\\ncopy\\n.\\ncopy\\n(\\nsoup\\n.\\np\\n)\\nprint\\n(\\np_copy\\n)\\n# <p>I want <b>pizza</b> and more <b>pizza</b>!</p>\\nThe copy is considered equal to the original, since it represents the\\nsame markup as the original, but it’s not the same object:\\nprint\\n(\\nsoup\\n.\\np\\n==\\np_copy\\n)\\n# True\\nprint\\n(\\nsoup\\n.\\np\\nis\\np_copy\\n)\\n# False\\nThe only real difference is that the copy is completely detached from\\nthe original Beautiful Soup object tree, just as if\\nextract()\\nhad\\nbeen called on it:\\nprint\\n(\\np_copy\\n.\\nparent\\n)\\n# None\\nThis is because two different\\nTag\\nobjects can’t occupy the same\\nspace at the same time.\\nAdvanced parser customization\\n¶\\nBeautiful Soup offers a number of ways to customize how the parser\\ntreats incoming HTML and XML. This section covers the most commonly\\nused customization techniques.\\nParsing only part of a document\\n¶\\nLet’s say you want to use Beautiful Soup to look at a document’s <a>\\ntags. It’s a waste of time and memory to parse the entire document and\\nthen go over it again looking for <a> tags. It would be much faster to\\nignore everything that wasn’t an <a> tag in the first place. The\\nSoupStrainer\\nclass allows you to choose which parts of an incoming\\ndocument are parsed. You just create a\\nSoupStrainer\\nand pass it in\\nto the\\nBeautifulSoup\\nconstructor as the\\nparse_only\\nargument.\\n(Note that\\nthis feature won’t work if you’re using the html5lib parser\\n.\\nIf you use html5lib, the whole document will be parsed, no\\nmatter what. This is because html5lib constantly rearranges the parse\\ntree as it works, and if some part of the document didn’t actually\\nmake it into the parse tree, it’ll crash. To avoid confusion, in the\\nexamples below I’ll be forcing Beautiful Soup to use Python’s\\nbuilt-in parser.)\\nclass\\nbs4.\\nSoupStrainer\\n¶\\nThe\\nSoupStrainer\\nclass takes the same arguments as a typical\\nmethod from\\nSearching the tree\\n:\\nname\\n,\\nattrs\\n,\\nstring\\n, and\\n**kwargs\\n. Here are\\nthree\\nSoupStrainer\\nobjects:\\nfrom\\nbs4\\nimport\\nSoupStrainer\\nonly_a_tags\\n=\\nSoupStrainer\\n(\\n\"a\"\\n)\\nonly_tags_with_id_link2\\n=\\nSoupStrainer\\n(\\nid\\n=\\n\"link2\"\\n)\\ndef\\nis_short_string\\n(\\nstring\\n):\\nreturn\\nstring\\nis\\nnot\\nNone\\nand\\nlen\\n(\\nstring\\n)\\n<\\n10\\nonly_short_strings\\n=\\nSoupStrainer\\n(\\nstring\\n=\\nis_short_string\\n)\\nI’m going to bring back the “three sisters” document one more time,\\nand we’ll see what the document looks like when it’s parsed with these\\nthree\\nSoupStrainer\\nobjects:\\nhtml_doc\\n=\\n\"\"\"<html><head><title>The Dormouse\\'s story</title></head>\\n<body>\\n<p class=\"title\"><b>The Dormouse\\'s story</b></p>\\n<p class=\"story\">Once upon a time there were three little sisters; and their names were\\n<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\\n<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\\n<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\\nand they lived at the bottom of a well.</p>\\n<p class=\"story\">...</p>\\n\"\"\"\\nprint\\n(\\nBeautifulSoup\\n(\\nhtml_doc\\n,\\n\"html.parser\"\\n,\\nparse_only\\n=\\nonly_a_tags\\n)\\n.\\nprettify\\n())\\n# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\\n# Elsie\\n# </a>\\n# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\\n# Lacie\\n# </a>\\n# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">\\n# Tillie\\n# </a>\\nprint\\n(\\nBeautifulSoup\\n(\\nhtml_doc\\n,\\n\"html.parser\"\\n,\\nparse_only\\n=\\nonly_tags_with_id_link2\\n)\\n.\\nprettify\\n())\\n# <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\\n# Lacie\\n# </a>\\nprint\\n(\\nBeautifulSoup\\n(\\nhtml_doc\\n,\\n\"html.parser\"\\n,\\nparse_only\\n=\\nonly_short_strings\\n)\\n.\\nprettify\\n())\\n# Elsie\\n# ,\\n# Lacie\\n# and\\n# Tillie\\n# ...\\n#\\nThe\\nSoupStrainer\\nbehavior is as follows:\\nWhen a tag matches, it is kept (including all its contents, whether they also\\nmatch or not).\\nWhen a tag does not match, the tag itself is not kept, but parsing continues\\ninto its contents to look for other tags that do match.\\nYou can also pass a\\nSoupStrainer\\ninto any of the methods covered\\nin\\nSearching the tree\\n. This probably isn’t terribly useful, but I\\nthought I’d mention it:\\nsoup\\n=\\nBeautifulSoup\\n(\\nhtml_doc\\n,\\n\\'html.parser\\'\\n)\\nsoup\\n.\\nfind_all\\n(\\nonly_short_strings\\n)\\n# [\\'\\\\n\\\\n\\', \\'\\\\n\\\\n\\', \\'Elsie\\', \\',\\\\n\\', \\'Lacie\\', \\' and\\\\n\\', \\'Tillie\\',\\n# \\'\\\\n\\\\n\\', \\'...\\', \\'\\\\n\\']\\nCustomizing multi-valued attributes\\n¶\\nIn an HTML document, an attribute like\\nclass\\nis given a list of\\nvalues, and an attribute like\\nid\\nis given a single value, because\\nthe HTML specification treats those attributes differently:\\nmarkup\\n=\\n\\'<a class=\"cls1 cls2\" id=\"id1 id2\">\\'\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n)\\nsoup\\n.\\na\\n[\\n\\'class\\'\\n]\\n# [\\'cls1\\', \\'cls2\\']\\nsoup\\n.\\na\\n[\\n\\'id\\'\\n]\\n# \\'id1 id2\\'\\nYou can turn this off by passing in\\nmulti_valued_attributes=None\\n. Than all attributes will be given a\\nsingle value:\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n,\\nmulti_valued_attributes\\n=\\nNone\\n)\\nsoup\\n.\\na\\n[\\n\\'class\\'\\n]\\n# \\'cls1 cls2\\'\\nsoup\\n.\\na\\n[\\n\\'id\\'\\n]\\n# \\'id1 id2\\'\\nYou can customize this behavior quite a bit by passing in a\\ndictionary for\\nmulti_valued_attributes\\n. If you need this, look at\\nHTMLTreeBuilder.DEFAULT_CDATA_LIST_ATTRIBUTES\\nto see the\\nconfiguration Beautiful Soup uses by default, which is based on the\\nHTML specification.\\n(This is a new feature in Beautiful Soup 4.8.0.)\\nHandling duplicate attributes\\n¶\\nWhen using the\\nhtml.parser\\nparser, you can use the\\non_duplicate_attribute\\nconstructor argument to customize what\\nBeautiful Soup does when it encounters a tag that defines the same\\nattribute more than once:\\nmarkup\\n=\\n\\'<a href=\"http://url1/\" href=\"http://url2/\">\\'\\nThe default behavior is to use the last value found for the tag:\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n)\\nsoup\\n.\\na\\n[\\n\\'href\\'\\n]\\n# http://url2/\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n,\\non_duplicate_attribute\\n=\\n\\'replace\\'\\n)\\nsoup\\n.\\na\\n[\\n\\'href\\'\\n]\\n# http://url2/\\nWith\\non_duplicate_attribute=\\'ignore\\'\\nyou can tell Beautiful Soup\\nto use the\\nfirst\\nvalue found and ignore the rest:\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n,\\non_duplicate_attribute\\n=\\n\\'ignore\\'\\n)\\nsoup\\n.\\na\\n[\\n\\'href\\'\\n]\\n# http://url1/\\n(lxml and html5lib always do it this way; their behavior can’t be\\nconfigured from within Beautiful Soup.)\\nIf you need more control, you can pass in a function that’s called on each\\nduplicate value:\\ndef\\naccumulate\\n(\\nattributes_so_far\\n,\\nkey\\n,\\nvalue\\n):\\nif\\nnot\\nisinstance\\n(\\nattributes_so_far\\n[\\nkey\\n],\\nlist\\n):\\nattributes_so_far\\n[\\nkey\\n]\\n=\\n[\\nattributes_so_far\\n[\\nkey\\n]]\\nattributes_so_far\\n[\\nkey\\n]\\n.\\nappend\\n(\\nvalue\\n)\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n,\\non_duplicate_attribute\\n=\\naccumulate\\n)\\nsoup\\n.\\na\\n[\\n\\'href\\'\\n]\\n# [\"http://url1/\", \"http://url2/\"]\\n(This is a new feature in Beautiful Soup 4.9.1.)\\nInstantiating custom subclasses\\n¶\\nWhen a parser tells Beautiful Soup about a tag or a string, Beautiful\\nSoup will instantiate a\\nTag\\nor\\nNavigableString\\nobject to\\ncontain that information. Instead of that default behavior, you can\\ntell Beautiful Soup to instantiate\\nsubclasses\\nof\\nTag\\nor\\nNavigableString\\n, subclasses you define with custom behavior:\\nfrom\\nbs4\\nimport\\nTag\\n,\\nNavigableString\\nclass\\nMyTag\\n(\\nTag\\n):\\npass\\nclass\\nMyString\\n(\\nNavigableString\\n):\\npass\\nmarkup\\n=\\n\"<div>some text</div>\"\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n)\\nisinstance\\n(\\nsoup\\n.\\ndiv\\n,\\nMyTag\\n)\\n# False\\nisinstance\\n(\\nsoup\\n.\\ndiv\\n.\\nstring\\n,\\nMyString\\n)\\n# False\\nmy_classes\\n=\\n{\\nTag\\n:\\nMyTag\\n,\\nNavigableString\\n:\\nMyString\\n}\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\\'html.parser\\'\\n,\\nelement_classes\\n=\\nmy_classes\\n)\\nisinstance\\n(\\nsoup\\n.\\ndiv\\n,\\nMyTag\\n)\\n# True\\nisinstance\\n(\\nsoup\\n.\\ndiv\\n.\\nstring\\n,\\nMyString\\n)\\n# True\\nThis can be useful when incorporating Beautiful Soup into a test\\nframework.\\n(This is a new feature in Beautiful Soup 4.8.1.)\\nTroubleshooting\\n¶\\ndiagnose()\\n¶\\nIf you’re having trouble understanding what Beautiful Soup does to a\\ndocument, pass the document into the\\ndiagnose()\\nfunction. (This function is new in\\nBeautiful Soup 4.2.0.) Beautiful Soup will print out a report showing\\nyou how different parsers handle the document, and tell you if you’re\\nmissing a parser that Beautiful Soup could be using:\\nfrom\\nbs4.diagnose\\nimport\\ndiagnose\\nwith\\nopen\\n(\\n\"bad.html\"\\n)\\nas\\nfp\\n:\\ndata\\n=\\nfp\\n.\\nread\\n()\\ndiagnose\\n(\\ndata\\n)\\n# Diagnostic running on Beautiful Soup 4.2.0\\n# Python version 2.7.3 (default, Aug 1 2012, 05:16:07)\\n# I noticed that html5lib is not installed. Installing it may help.\\n# Found lxml version 2.3.2.0\\n#\\n# Trying to parse your data with html.parser\\n# Here\\'s what html.parser did with the document:\\n# ...\\nJust looking at the output of diagnose() might show you how to solve the\\nproblem. Even if not, you can paste the output of\\ndiagnose()\\nwhen\\nasking for help.\\nErrors when parsing a document\\n¶\\nThere are two different kinds of parse errors. There are crashes,\\nwhere you feed a document to Beautiful Soup and it raises an\\nexception (usually an\\nHTMLParser.HTMLParseError\\n). And there is\\nunexpected behavior, where a Beautiful Soup parse tree looks a lot\\ndifferent than the document used to create it.\\nThese problems are almost never problems with Beautiful Soup itself.\\nThis is not because Beautiful Soup is an amazingly well-written piece\\nof software. It’s because Beautiful Soup doesn’t include any parsing\\ncode. Instead, it relies on external parsers. If one parser isn’t\\nworking on a certain document, the best solution is to try a different\\nparser. See\\nInstalling a parser\\nfor details and a parser\\ncomparison. If this doesn’t help, you might need to inspect the\\ndocument tree found inside the\\nBeautifulSoup\\nobject, to see where\\nthe markup you’re looking for actually ended up.\\nVersion mismatch problems\\n¶\\nSyntaxError:\\nInvalid\\nsyntax\\n(on the line\\nROOT_TAG_NAME\\n=\\n\\'[document]\\'\\n): Caused by running an old Python 2 version of\\nBeautiful Soup under Python 3, without converting the code.\\nImportError:\\nNo\\nmodule\\nnamed\\nHTMLParser\\n- Caused by running an old\\nPython 2 version of Beautiful Soup under Python 3.\\nImportError:\\nNo\\nmodule\\nnamed\\nhtml.parser\\n- Caused by running the\\nPython 3 version of Beautiful Soup under Python 2.\\nImportError:\\nNo\\nmodule\\nnamed\\nBeautifulSoup\\n- Caused by running\\nBeautiful Soup 3 code in an environment that doesn’t have BS3\\ninstalled. Or, by writing Beautiful Soup 4 code without knowing that\\nthe package name has changed to\\nbs4\\n.\\nImportError:\\nNo\\nmodule\\nnamed\\nbs4\\n- Caused by running Beautiful\\nSoup 4 code in an environment that doesn’t have BS4 installed.\\nParsing XML\\n¶\\nBy default, Beautiful Soup parses documents as HTML. To parse a\\ndocument as XML, pass in “xml” as the second argument to the\\nBeautifulSoup\\nconstructor:\\nsoup\\n=\\nBeautifulSoup\\n(\\nmarkup\\n,\\n\"xml\"\\n)\\nYou’ll need to\\nhave lxml installed\\n.\\nOther parser problems\\n¶\\nIf your script works on one computer but not another, or in one\\nvirtual environment but not another, or outside the virtual\\nenvironment but not inside, it’s probably because the two\\nenvironments have different parser libraries available. For example,\\nyou may have developed the script on a computer that has lxml\\ninstalled, and then tried to run it on a computer that only has\\nhtml5lib installed. See\\nDifferences between parsers\\nfor why this\\nmatters, and fix the problem by mentioning a specific parser library\\nin the\\nBeautifulSoup\\nconstructor.\\nBecause\\nHTML tags and attributes are case-insensitive\\n, all three HTML\\nparsers convert tag and attribute names to lowercase. That is, the\\nmarkup <TAG></TAG> is converted to <tag></tag>. If you want to\\npreserve mixed-case or uppercase tags and attributes, you’ll need to\\nparse the document as XML.\\nMiscellaneous\\n¶\\nUnicodeEncodeError:\\n\\'charmap\\'\\ncodec\\ncan\\'t\\nencode\\ncharacter\\n\\'\\\\xfoo\\'\\nin\\nposition\\nbar\\n(or just about any other\\nUnicodeEncodeError\\n) - This problem shows up in two main\\nsituations. First, when you try to print a Unicode character that\\nyour console doesn’t know how to display. (See\\nthis page on the\\nPython wiki\\nfor help.)\\nSecond, when you’re writing to a file and you pass in a Unicode\\ncharacter that’s not supported by your default encoding. In this\\ncase, the simplest solution is to explicitly encode the Unicode\\nstring into UTF-8 with\\nu.encode(\"utf8\")\\n.\\nKeyError:\\n[attr]\\n- Caused by accessing\\ntag[\\'attr\\']\\nwhen the\\ntag in question doesn’t define the\\nattr\\nattribute. The most\\ncommon errors are\\nKeyError:\\n\\'href\\'\\nand\\nKeyError:\\n\\'class\\'\\n.\\nUse\\ntag.get(\\'attr\\')\\nif you’re not sure\\nattr\\nis\\ndefined, just as you would with a Python dictionary.\\nAttributeError:\\n\\'ResultSet\\'\\nobject\\nhas\\nno\\nattribute\\n\\'foo\\'\\n- This\\nusually happens because you expected\\nfind_all()\\nto return a\\nsingle tag or string. But\\nfind_all()\\nreturns a\\nlist\\nof tags\\nand strings–a\\nResultSet\\nobject. You need to iterate over the\\nlist and look at the\\n.foo\\nof each one. Or, if you really only\\nwant one result, you need to use\\nfind()\\ninstead of\\nfind_all()\\n.\\nAttributeError:\\n\\'NoneType\\'\\nobject\\nhas\\nno\\nattribute\\n\\'foo\\'\\n- This\\nusually happens because you called\\nfind()\\nand then tried to\\naccess the\\n.foo`\\nattribute of the result. But in your case,\\nfind()\\ndidn’t find anything, so it returned\\nNone\\n, instead of\\nreturning a tag or a string. You need to figure out why your\\nfind()\\ncall isn’t returning anything.\\nAttributeError:\\n\\'NavigableString\\'\\nobject\\nhas\\nno\\nattribute\\n\\'foo\\'\\n- This usually happens because you’re treating a string as\\nthough it were a tag. You may be iterating over a list, expecting\\nthat it contains nothing but tags, when it actually contains both tags and\\nstrings.\\nImproving Performance\\n¶\\nBeautiful Soup will never be as fast as the parsers it sits on top\\nof. If response time is critical, if you’re paying for computer time\\nby the hour, or if there’s any other reason why computer time is more\\nvaluable than programmer time, you should forget about Beautiful Soup\\nand work directly atop\\nlxml\\n.\\nThat said, there are things you can do to speed up Beautiful Soup. If\\nyou’re not using lxml as the underlying parser, my advice is to\\nstart\\n. Beautiful Soup parses documents\\nsignificantly faster using lxml than using html.parser or html5lib.\\nYou can speed up encoding detection significantly by installing the\\ncchardet\\nlibrary.\\nParsing only part of a document\\nwon’t save you much time parsing\\nthe document, but it can save a lot of memory, and it’ll make\\nsearching\\nthe document much faster.\\nTranslating this documentation\\n¶\\nNew translations of the Beautiful Soup documentation are greatly\\nappreciated. Translations should be licensed under the MIT license,\\njust like Beautiful Soup and its English documentation are.\\nThere are two ways of getting your translation into the main code base\\nand onto the Beautiful Soup website:\\nCreate a branch of the Beautiful Soup repository, add your\\ntranslation, and propose a merge with the main branch, the same\\nas you would do with a proposed change to the source code.\\nSend a message to the Beautiful Soup discussion group with a link to\\nyour translation, or attach your translation to the message.\\nUse the Chinese or Brazilian Portuguese translations as your model. In\\nparticular, please translate the source file\\ndoc/source/index.rst\\n,\\nrather than the HTML version of the documentation. This makes it\\npossible to publish the documentation in a variety of formats, not\\njust HTML.\\nBeautiful Soup 3\\n¶\\nBeautiful Soup 3 is the previous release series, and is no longer\\nsupported. Development of Beautiful Soup 3 stopped in 2012, and the\\npackage was completely discontinued in 2021. There’s no reason to\\ninstall it unless you’re trying to get very old software to work, but\\nit’s published through PyPi as\\nBeautifulSoup\\n:\\n$\\npip\\ninstall\\nBeautifulSoup\\nYou can also download\\na tarball of the final release, 3.2.2\\n.\\nIf you ran\\npip\\ninstall\\nbeautifulsoup\\nor\\npip\\ninstall\\nBeautifulSoup\\n, but your code doesn’t work, you installed Beautiful\\nSoup 3 by mistake. You need to run\\npip\\ninstall\\nbeautifulsoup4\\n.\\nThe documentation for Beautiful Soup 3 is archived online\\n.\\nPorting code to BS4\\n¶\\nMost code written against Beautiful Soup 3 will work against Beautiful\\nSoup 4 with one simple change. All you should have to do is change the\\npackage name from\\nBeautifulSoup\\nto\\nbs4\\n. So this:\\nfrom\\nBeautifulSoup\\nimport\\nBeautifulSoup\\nbecomes this:\\nfrom\\nbs4\\nimport\\nBeautifulSoup\\nIf you get the\\nImportError\\n“No module named BeautifulSoup”, your\\nproblem is that you’re trying to run Beautiful Soup 3 code, but you\\nonly have Beautiful Soup 4 installed.\\nIf you get the\\nImportError\\n“No module named bs4”, your problem\\nis that you’re trying to run Beautiful Soup 4 code, but you only\\nhave Beautiful Soup 3 installed.\\nAlthough BS4 is mostly backward-compatible with BS3, most of its\\nmethods have been deprecated and given new names for\\nPEP 8 compliance\\n. There are numerous other\\nrenames and changes, and a few of them break backward compatibility.\\nHere’s what you’ll need to know to convert your BS3 code and habits to BS4:\\nYou need a parser\\n¶\\nBeautiful Soup 3 used Python’s\\nSGMLParser\\n, a module that was\\ndeprecated and removed in Python 3.0. Beautiful Soup 4 uses\\nhtml.parser\\nby default, but you can plug in lxml or html5lib and\\nuse that instead. See\\nInstalling a parser\\nfor a comparison.\\nSince\\nhtml.parser\\nis not the same parser as\\nSGMLParser\\n, you\\nmay find that Beautiful Soup 4 gives you a different parse tree than\\nBeautiful Soup 3 for the same markup. If you swap out\\nhtml.parser\\nfor lxml or html5lib, you may find that the parse tree changes yet\\nagain. If this happens, you’ll need to update your scraping code to\\nprocess the new tree.\\nMethod names\\n¶\\nrenderContents\\n->\\nencode_contents\\nreplaceWith\\n->\\nreplace_with\\nreplaceWithChildren\\n->\\nunwrap\\nfindAll\\n->\\nfind_all\\nfindAllNext\\n->\\nfind_all_next\\nfindAllPrevious\\n->\\nfind_all_previous\\nfindNext\\n->\\nfind_next\\nfindNextSibling\\n->\\nfind_next_sibling\\nfindNextSiblings\\n->\\nfind_next_siblings\\nfindParent\\n->\\nfind_parent\\nfindParents\\n->\\nfind_parents\\nfindPrevious\\n->\\nfind_previous\\nfindPreviousSibling\\n->\\nfind_previous_sibling\\nfindPreviousSiblings\\n->\\nfind_previous_siblings\\ngetText\\n->\\nget_text\\nnextSibling\\n->\\nnext_sibling\\npreviousSibling\\n->\\nprevious_sibling\\nSome arguments to the Beautiful Soup constructor were renamed for the\\nsame reasons:\\nBeautifulSoup(parseOnlyThese=...)\\n->\\nBeautifulSoup(parse_only=...)\\nBeautifulSoup(fromEncoding=...)\\n->\\nBeautifulSoup(from_encoding=...)\\nI renamed one method for compatibility with Python 3:\\nTag.has_key()\\n->\\nTag.has_attr()\\nI renamed one attribute to use more accurate terminology:\\nTag.isSelfClosing\\n->\\nTag.is_empty_element\\nI renamed three attributes to avoid using words that have special\\nmeaning to Python. Unlike the others, these changes are\\nnot backwards\\ncompatible.\\nIf you used these attributes in BS3, your code will break\\nin BS4 until you change them.\\nUnicodeDammit.unicode\\n->\\nUnicodeDammit.unicode_markup\\nTag.next\\n->\\nTag.next_element\\nTag.previous\\n->\\nTag.previous_element\\nThese methods are left over from the Beautiful Soup 2 API. They’ve\\nbeen deprecated since 2006 and should not be used at all:\\nTag.fetchNextSiblings\\nTag.fetchPreviousSiblings\\nTag.fetchPrevious\\nTag.fetchPreviousSiblings\\nTag.fetchParents\\nTag.findChild\\nTag.findChildren\\nGenerators\\n¶\\nI gave the generators PEP 8-compliant names, and transformed them into\\nproperties:\\nchildGenerator()\\n->\\nchildren\\nnextGenerator()\\n->\\nnext_elements\\nnextSiblingGenerator()\\n->\\nnext_siblings\\npreviousGenerator()\\n->\\nprevious_elements\\npreviousSiblingGenerator()\\n->\\nprevious_siblings\\nrecursiveChildGenerator()\\n->\\ndescendants\\nparentGenerator()\\n->\\nparents\\nSo instead of this:\\nfor\\nparent\\nin\\ntag\\n.\\nparentGenerator\\n():\\n...\\nYou can write this:\\nfor\\nparent\\nin\\ntag\\n.\\nparents\\n:\\n...\\n(But the old code will still work.)\\nSome of the generators used to yield\\nNone\\nafter they were done, and\\nthen stop. That was a bug. Now the generators just stop.\\nThere are two new generators,\\n.strings and\\n.stripped_strings\\n.\\n.strings\\nyields\\nNavigableString objects, and\\n.stripped_strings\\nyields Python\\nstrings that have had whitespace stripped.\\nXML\\n¶\\nThere is no longer a\\nBeautifulStoneSoup\\nclass for parsing XML. To\\nparse XML you pass in “xml” as the second argument to the\\nBeautifulSoup\\nconstructor. For the same reason, the\\nBeautifulSoup\\nconstructor no longer recognizes the\\nisHTML\\nargument.\\nBeautiful Soup’s handling of empty-element XML tags has been\\nimproved. Previously when you parsed XML you had to explicitly say\\nwhich tags were considered empty-element tags. The\\nselfClosingTags\\nargument to the constructor is no longer recognized. Instead,\\nBeautiful Soup considers any empty tag to be an empty-element tag. If\\nyou add a child to an empty-element tag, it stops being an\\nempty-element tag.\\nEntities\\n¶\\nAn incoming HTML or XML entity is always converted into the\\ncorresponding Unicode character. Beautiful Soup 3 had a number of\\noverlapping ways of dealing with entities, which have been\\nremoved. The\\nBeautifulSoup\\nconstructor no longer recognizes the\\nsmartQuotesTo\\nor\\nconvertEntities\\narguments. (\\nUnicode,\\nDammit\\nstill has\\nsmart_quotes_to\\n, but its default is now to turn\\nsmart quotes into Unicode.) The constants\\nHTML_ENTITIES\\n,\\nXML_ENTITIES\\n, and\\nXHTML_ENTITIES\\nhave been removed, since they\\nconfigure a feature (transforming some but not all entities into\\nUnicode characters) that no longer exists.\\nIf you want to turn Unicode characters back into HTML entities on\\noutput, rather than turning them into UTF-8 characters, you need to\\nuse an\\noutput formatter\\n.\\nMiscellaneous\\n¶\\nTag.string\\nnow operates recursively. If tag A\\ncontains a single tag B and nothing else, then A.string is the same as\\nB.string. (Previously, it was None.)\\nMulti-valued attributes\\nlike\\nclass\\nhave lists of strings as\\ntheir values, not simple strings. This may affect the way you search by CSS\\nclass.\\nTag\\nobjects now implement the\\n__hash__\\nmethod, such that two\\nTag\\nobjects are considered equal if they generate the same\\nmarkup. This may change your script’s behavior if you put\\nTag\\nobjects into a dictionary or set.\\nIf you pass one of the\\nfind*\\nmethods both\\nstring\\nand\\na tag-specific argument like\\nname\\n, Beautiful Soup will\\nsearch for tags that match your tag-specific criteria and whose\\nTag.string\\nmatches your\\nstring\\nvalue. It will\\nnot\\nfind the strings themselves. Previously,\\nBeautiful Soup ignored the tag-specific arguments and looked for\\nstrings.\\nThe\\nBeautifulSoup\\nconstructor no longer recognizes the\\nmarkupMassage\\nargument. It’s now the parser’s responsibility to\\nhandle markup correctly.\\nThe rarely-used alternate parser classes like\\nICantBelieveItsBeautifulSoup\\nand\\nBeautifulSOAP\\nhave been\\nremoved. It’s now the parser’s decision how to handle ambiguous\\nmarkup.\\nThe\\nprettify()\\nmethod now returns a Unicode string, not a bytestring.\\nTable of Contents\\nBeautiful Soup Documentation\\nGetting help\\nQuick Start\\nInstalling Beautiful Soup\\nInstalling a parser\\nMaking the soup\\nKinds of objects\\nTag\\nTag.name\\nTag.attrs\\nNavigableString\\nBeautifulSoup\\nSpecial strings\\nComment\\nFor HTML documents\\nStylesheet\\nScript\\nTemplate\\nFor XML documents\\nDeclaration\\nDoctype\\nCData\\nProcessingInstruction\\nNavigating the tree\\nGoing down\\nNavigating using tag names\\n.contents\\nand\\n.children\\n.descendants\\n.string\\n.strings\\nand\\nstripped_strings\\nGoing up\\n.parent\\n.parents\\nGoing sideways\\n.next_sibling\\nand\\n.previous_sibling\\n.next_siblings\\nand\\n.previous_siblings\\nGoing back and forth\\n.next_element\\nand\\n.previous_element\\n.next_elements\\nand\\n.previous_elements\\nSearching the tree\\nKinds of filters\\nA string\\nA regular expression\\nTrue\\nA function\\nA list\\nfind_all()\\nThe\\nname\\nargument\\nThe keyword arguments\\nSearching by CSS class\\nThe\\nstring\\nargument\\nThe\\nlimit\\nargument\\nThe\\nrecursive\\nargument\\nCalling a tag is like calling\\nfind_all()\\nfind()\\nfind_parents()\\nand\\nfind_parent()\\nfind_next_siblings()\\nand\\nfind_next_sibling()\\nfind_previous_siblings()\\nand\\nfind_previous_sibling()\\nfind_all_next()\\nand\\nfind_next()\\nfind_all_previous()\\nand\\nfind_previous()\\nCSS selectors through the\\n.css\\nproperty\\nAdvanced Soup Sieve features\\nNamespaces in CSS selectors\\nHistory of CSS selector support\\nModifying the tree\\nChanging tag names and attributes\\nModifying\\n.string\\nappend()\\nextend()\\nNavigableString()\\nand\\n.new_tag()\\ninsert()\\ninsert_before()\\nand\\ninsert_after()\\nclear()\\nextract()\\ndecompose()\\nreplace_with()\\nwrap()\\nunwrap()\\nsmooth()\\nOutput\\nPretty-printing\\nNon-pretty printing\\nOutput formatters\\nFormatter objects\\nHTMLFormatter\\nXMLFormatter\\nWriting your own formatter\\nget_text()\\nSpecifying the parser to use\\nDifferences between parsers\\nEncodings\\nOutput encoding\\nUnicode, Dammit\\nSmart quotes\\nInconsistent encodings\\nLine numbers\\nComparing objects for equality\\nCopying Beautiful Soup objects\\nAdvanced parser customization\\nParsing only part of a document\\nSoupStrainer\\nCustomizing multi-valued attributes\\nHandling duplicate attributes\\nInstantiating custom subclasses\\nTroubleshooting\\ndiagnose()\\nErrors when parsing a document\\nVersion mismatch problems\\nParsing XML\\nOther parser problems\\nMiscellaneous\\nImproving Performance\\nTranslating this documentation\\nBeautiful Soup 3\\nPorting code to BS4\\nYou need a parser\\nMethod names\\nGenerators\\nXML\\nEntities\\nMiscellaneous\\nThis Page\\nShow Source\\nQuick search\\nNavigation\\nindex\\nmodules\\n|\\nBeautiful Soup 4.12.0 documentation\\n»\\nBeautiful Soup Documentation\\n© Copyright 2004-2023, Leonard Richardson.\\nCreated using\\nSphinx\\n7.2.6.')]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "# Directory containing the files\n",
        "directory_path = \"/content/drive/MyDrive/content/\"\n",
        "\n",
        "# Initialize an empty list to store pages from all files\n",
        "pages = []\n",
        "\n",
        "# Iterate over all files in the directory\n",
        "for filename in os.listdir(directory_path):\n",
        "    file_path = os.path.join(directory_path, filename)\n",
        "\n",
        "    # Check if the file is a text file\n",
        "    if os.path.isfile(file_path) and filename.endswith(\".txt\"):\n",
        "        loader = TextLoader(file_path)\n",
        "        for page in loader.lazy_load():\n",
        "            pages.append(page)\n",
        "\n",
        "# Now all_pages contains the content of all .txt files in the directory\n",
        "print(pages)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vfCNUf4i3dPl",
        "outputId": "8f77cab7-5759-4d70-dc2b-4194264105dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "z-OWwPY2Z5g_"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "# docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(pages)\n",
        "vectorstore = InMemoryVectorStore.from_documents(\n",
        "    documents=splits, embedding=embeddings\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mNdvePhVQ7Rz",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "813d0a9b-56d8-4b19-97fd-dd7290c301e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'page': 0}, page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.'),\n",
              " Document(metadata={'source': '/content/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'page': 1}, page_content='Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\\nof continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\\nsequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2'),\n",
              " Document(metadata={'source': '/content/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'page': 2}, page_content='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3'),\n",
              " Document(metadata={'source': '/content/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'page': 3}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\\nthe matrix of outputs as:\\nAttention(Q,K,V ) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = ∑dk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
              " Document(metadata={'source': '/content/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'page': 4}, page_content='MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\\nwhere headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈Rdmodel×dk , WK\\ni ∈Rdmodel×dk , WV\\ni ∈Rdmodel×dv\\nand WO ∈Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0,xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5'),\n",
              " Document(metadata={'source': '/content/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'page': 5}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 ·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel )\\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6'),\n",
              " Document(metadata={'source': '/content/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'page': 6}, page_content='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 15], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+ n·d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate= d−0.5\\nmodel ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_stepstraining steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps= 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\nResidual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\n7'),\n",
              " Document(metadata={'source': '/content/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'page': 7}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0 ·1020\\nGNMT + RL [31] 24.6 39.92 2.3 ·1019 1.4 ·1020\\nConvS2S [8] 25.16 40.46 9.6 ·1018 1.5 ·1020\\nMoE [26] 26.03 40.56 2.0 ·1019 1.2 ·1020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0 ·1020\\nGNMT + RL Ensemble [31] 26.30 41.16 1.8 ·1020 1.1 ·1021\\nConvS2S Ensemble [8] 26.36 41.29 7.7 ·1019 1.2 ·1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.0 2.3 ·1019\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α= 0.6 [31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
              " Document(metadata={'source': '/content/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'page': 8}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'),\n",
              " Document(metadata={'source': '/content/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'page': 9}, page_content='References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10'),\n",
              " Document(metadata={'source': '/content/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'page': 10}, page_content='[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11')]"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "GfybMbfrb3MW",
        "outputId": "2fb790e9-17a0-48f0-9e8b-3caea8603528"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2'"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever.invoke({\"input\": \"What is encoder?\"})[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "83Pl-SXfbbBD"
      },
      "outputs": [],
      "source": [
        "def rag_call(query):\n",
        "  retrieved_docs=retriever.invoke({\"input\":query})\n",
        "  print(retrieved_docs)\n",
        "  context=[doc.page_content for doc in retrieved_docs]\n",
        "\n",
        "  system_prompt=\"\"\"\n",
        "  You are a question answering bot. You will be provided context and a query,your task is to answer the query based on the context only.\n",
        "  Keep your answer precise and grounded. If you dont find answer to the question respond with 'Sorry I dont know the answer.'\n",
        "  \"\"\"\n",
        "\n",
        "  messages = [\n",
        "      {\n",
        "          \"role\":\"system\",\n",
        "          \"content\": system_prompt\n",
        "      },\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Query: {query} \\n Context: {context}\"\n",
        "      }\n",
        "    ]\n",
        "\n",
        "  response=llm_generate(messages)\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "z-1nkPMkYeEL",
        "outputId": "75503a5a-5180-4e88-ab0f-c35d1d776d34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(id='16532145-83b5-4fed-acf3-cf59191f528d', metadata={'source': '/content/drive/MyDrive/content/Sarang Punekar, first transgender student of SPPU in Pune, dies by suicide _ Pune News - The Indian Express.txt'}, page_content='Punekar had worked as a regional coordinator, north\\nMaharashtra\\n, for Pune-headquartered NGO Samyak, which works for women and sexuality. Anand Pawar, executive director of the NGO, said she was first referred to Samyak as an intern but subsequently she was absorbed. “In the development sector it is always the norm to get transgender people to work in HIV prevention programmes. But she broke the stereotype and worked for abortion rights. As the coordinator she executed the project successfully,” he said.\\nIn the course of her work, she had to coordinate with government officials, doctors and NGOs. The project got over in 2020 and Punekar decided to relocate to Rajasthan. “She said she wanted to be with the community there,” Pawar said.\\nAdvertisement\\nTransgender poetess Disha Pinky Sheikh was her professor and personal friend. “She was a very strong voice. Her death marks the end of a very different kind of support for the community,” she said.\\nDiscover the Benefits of Our Subscription!'), Document(id='a005c974-df35-4034-b8b5-ddf26c631822', metadata={'source': '/content/drive/MyDrive/content/Sarang Punekar, first transgender student of SPPU in Pune, dies by suicide _ Pune News - The Indian Express.txt'}, page_content='Advertisement\\nTambe, while speaking to\\nThe Indian Express\\non Thursday at Punekar’s funeral, felt society at large failed to help Punekar realise her dreams.\\nShiv Sena\\n(UBT) leader Sushma Andhare was among those who had gathered to bid farewell to Punekar, who had made a name for herself within a short time as a passionate speaker and advocate for gender rights and other causes. “As a student, Punekar brought about new perspectives to gender studies. She wanted to create knowledge and wanted to do original work about the language and customs of her community. It is our failure as a society that we could not support her dreams,” she said.\\nPunekar had worked as a regional coordinator, north\\nMaharashtra'), Document(id='14503ca8-71cb-4dba-b8b8-192672e6b6ca', metadata={'source': '/content/drive/MyDrive/content/Sarang Punekar, first transgender student of SPPU in Pune, dies by suicide _ Pune News - The Indian Express.txt'}, page_content=', she was living among the transgender community there and worked for them. “We had asked her to come back,” said\\nPune\\n-based freelance writer Ashwini Satav, who worked with her for a long time.\\nPunekar was a strong supporter of the Ambedkarite movement and also raised her voice against\\nNRC\\nand CAA. “Sarang was brilliant in her analysis of caste and power hierarchy. As the first transgender student of the university, her very presence was a unique experience for us, both as academics as well as administrators,” Dr Anagha Tambe, head of the Department of Women’s Studies at SPPU which Punekar had joined in 2018, said.\\nAdvertisement\\nTambe, while speaking to\\nThe Indian Express\\non Thursday at Punekar’s funeral, felt society at large failed to help Punekar realise her dreams.\\nShiv Sena'), Document(id='04e9a6bf-a20e-4d06-a0cc-685fd5023a85', metadata={'source': '/content/drive/MyDrive/content/Sarang Punekar, first transgender student of SPPU in Pune, dies by suicide _ Pune News - The Indian Express.txt'}, page_content=\"Trending\\nSchoolboy's electrifying dance to Devara's 'Davaudi' goes viral; earns praise from Jr NTR\\nTrending\\nA shoe or a clutch? Influencer reacts to Balenciaga’s new product\\nTrending\\nRam Gopal Varma shares AI video reimagining 'The Substance' with him, Sridevi and Jahnvi Kapoor: 'AI is becoming too much'\\nJan 17:\\nLatest News\\n01\\nPune’s Jewish community heaves a sigh of relief as Israel & Hamas agree to a ceasefire\\n02\\nWorking on new start-up policy, plans to set up ‘Innovation City’: CM Devendra Fadnavis\\n03\\nVijay Hazare Trophy: Karun Nair gives selectors more food for thought as Vidarbha thrash Maharashtra to enter final\\n04\\nEmergency brazenly undermined Dr B R Ambedkar’s Indian Constitution: CM Devendra Fadnavis\\n05\\nPune firm CEO lured by ‘Facebook friend’, loses Rs 42 lakh in share trading fraud\\nAdvertisement\\nLink Subscription\\nSubscribe\\nSign In\\ne-paper\\nPremium\\nIndia\\nElections 2024\\nBollywood\\nOpinion\\nPolitical Pulse\\nExplained\\nScience\\nCricket\\nSports\\nWorld\\nBusiness\\nEntertainment\\nJobs\\nHealth\")]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Mahatma Gandhi is not mentioned in the provided context. The context discusses Sarang Punekar, a transgender activist and scholar, who worked for women and sexuality rights, and was a strong supporter of the Ambedkarite movement.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "rag_call(\"who is mahatma gandhi?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIm_H9T1hIn_"
      },
      "outputs": [],
      "source": [
        "# def mm_rag_call(query,image_path):\n",
        "#   import PIL.Image\n",
        "#   sample_file_1 = PIL.Image.open(image_path)\n",
        "\n",
        "#   prompt = \"Give summary of given image\"\n",
        "#   summary_response = model.generate_content([prompt, sample_file_1])\n",
        "\n",
        "#   retrieved_docs=retriever.invoke({\"input\":str(summary_response)})\n",
        "#   context=[doc.page_content for doc in retrieved_docs]\n",
        "\n",
        "#   system_prompt=\"\"\"\n",
        "#   You are a question answering bot. You will be provided context and a query,your task is to answer the query based on the context only.\n",
        "#   Keep your answer precise and grounded. If you dont find answer to the question respond with 'Sorry I dont know the answer.'\n",
        "#   \"\"\"\n",
        "\n",
        "#   messages = [\n",
        "#       {\n",
        "#           \"role\":\"system\",\n",
        "#           \"content\": system_prompt\n",
        "#       },\n",
        "#       {\n",
        "#         \"role\": \"user\",\n",
        "#         \"content\": f\"Query: {query} \\n Context: {context}\"\n",
        "#       }\n",
        "#     ]\n",
        "\n",
        "#   response=llm_generate(messages)\n",
        "#   return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "o-2LTqhSiKrK",
        "outputId": "b5997c78-289a-4751-ac41-44264e1ec07f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The given architecture appears to be based on the Transformer model, which was introduced in the paper \"Attention is All You Need\" by Vaswani et al. The context provides details about the architecture of the Transformer model, specifically focusing on attention mechanisms.\\n\\nThe main components of the architecture described in the context are:\\n\\n1. Layer normalization: Applied around each sub-layer in the model.\\n\\n2. Scaled Dot-Product Attention: A type of attention function used in the Transformer model. It computes attention by taking the dot product of the query with all keys, dividing each by the square root of the dimension of the keys, and applying a softmax function to obtain the weights on the values.\\n\\n3. Multi-Head Attention: Instead of performing a single attention function, the model linearly projects the queries, keys, and values multiple times (h times) with different learned projections. This allows the model to jointly attend to information from different representation subspaces at different positions.\\n\\n4. Encoder and Decoder Self-Attention: In the encoder, self-attention layers allow each position to attend to all positions in the previous layer of the encoder. Similarly, in the decoder, self-attention layers allow each position to attend to all positions in the decoder up to and including that position. However, leftward information flow is prevented in the decoder by masking out illegal connections in the scaled dot-product attention.\\n\\n5. Position-wise Feed-Forward Networks: Applied to each position separately and identically. They are used to introduce non-linearity into the model.'"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# mm_rag_call(\"Explain given architecture?\",'/content/arch.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqerAz3dik46"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}